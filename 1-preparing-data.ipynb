{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-part-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/23-keras-part-1/1-preparing-data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBYmIpCvos28",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrgL9VevovDD",
        "colab_type": "text"
      },
      "source": [
        "There are many fine deep-learning libraries out there, and each has its\n",
        "advantages. Rather than try to cover many libraries, we’ll focus on just\n",
        "one, called Keras. This library is powerful, easy to use, popular, free,\n",
        "and open-source.\n",
        "\n",
        "One of the nice things about working with Keras is that a typical session\n",
        "of building and training a machine-learning system requires very\n",
        "little routine Python programming. The actual deep learning code is\n",
        "often the easiest part of the program: we build the network with just a\n",
        "few lines, and train it with just one or two function calls. Most of the\n",
        "rest of the program is made of supporting tasks, such as getting the\n",
        "input data, cleaning it, structuring it for use in the network, writing\n",
        "routines for saving data and visualizing results, and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHbntjWxpbmS",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFprmAHupeXo",
        "colab_type": "code",
        "outputId": "c1e4bf64-7071-4e1a-99c5-8224aea07bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras import backend as Keras_backend\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as keras_backend\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvkDKyhnpKy",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hnik-tAnxg2",
        "colab_type": "text"
      },
      "source": [
        "How easy it is to load the MNIST set, since it’s provided\n",
        "with Keras. To get it, we import the mnist module and then use\n",
        "its custom load_data() function to get the data. \n",
        "\n",
        "This returns two lists:\n",
        " * the training data and \n",
        " * the test data. \n",
        " \n",
        "Each list in turn contains two lists,\n",
        "* holding the features (that is, the images), and \n",
        "* the labels. \n",
        "\n",
        "We can use Python’s convenient assignment mechanism to assign all four lists to\n",
        "our own variables with just one statement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCcMp-VVoZUL",
        "colab_type": "code",
        "outputId": "7b40cd34-8afb-4581-e763-601b1a84a270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# load MNIST data and save sizes\n",
        "(samples_train, labels_train), (samples_test, labels_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoVSayiFqbFw",
        "colab_type": "text"
      },
      "source": [
        "when we use a technique like cross-validation\n",
        "we break down our input data into the training set, the validation\n",
        "set, and the test set. We teach many variations of the system using\n",
        "the training set, and then after each training we evaluate the performance\n",
        "with the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw4czu6KrMHv",
        "colab_type": "text"
      },
      "source": [
        "This is telling us that samples_train is a 3D block of 60,000 layers.\n",
        "Each layer holds a 28 by 28 image. The labels_train variable is a 1D\n",
        "list of 60,000 elements (we’ll see that each is a number from 0 to 9).\n",
        "\n",
        "The extra comma at the end of (60000,) is a Python convention to tell\n",
        "us that this is a list of 60,000 elements, and not just the number 60,000\n",
        "surrounded by parentheses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wqdUKQreOO",
        "colab_type": "text"
      },
      "source": [
        "## Looking at the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_kkM0rrfSh",
        "colab_type": "text"
      },
      "source": [
        "There are at least two potential sources of problems to keep an eye\n",
        "out for. \n",
        "\n",
        "* Content problems are numerical issues with the data itself,\n",
        "* while structural problems are issues regarding how the data is organized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5-uOdfWt3BK",
        "colab_type": "text"
      },
      "source": [
        "### Content problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bctpOSb_t43K",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-1.JPG?raw=1' width='800'/>\n",
        "\n",
        "There are four standout issues.\n",
        "First, some of the images bleed very close to the edge of the 28 by 28\n",
        "box, rather than sitting inside a relatively thick black border of 4 pixels\n",
        "all around that the original paper describes.\n",
        "\n",
        "Some images from the MNIST training set that demonstrate\n",
        "a bleeding of the image very near, or right up to, the border. The\n",
        "number above each example shows its index in the training set.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-2.JPG?raw=1' width='800'/>\n",
        "\n",
        "Second, some of the digits appear to have had pieces cropped away,\n",
        "substantially changing their shape.Some images from the MNIST training set that have been\n",
        "cropped, chopping away some of what seems very likely to have been\n",
        "drawn, and sometimes creating multiple, disconnected pieces.\n",
        "\n",
        "Third, some of the images are noisy. Sometimes this means that lines\n",
        "thin out or disappear. More often there are spurious regions of white,\n",
        "perhaps due to errors during cropping or thresholding. These don’t\n",
        "usually cause much confusion to human observers, but these artifacts\n",
        "have the potential to throw off a computerized network.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-3.JPG?raw=1' width='800'/>\n",
        "\n",
        "Finally, there are some examples that seem challenging to interpret,\n",
        "either because of how they were drawn, or how they were processed.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-5.JPG?raw=1' width='800'/>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cwfL0Du7gAs",
        "colab_type": "text"
      },
      "source": [
        "### Structural problems "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sj3-l9S7ieI",
        "colab_type": "text"
      },
      "source": [
        "Our main interest is in the shapes of the variables that we got from\n",
        "mnist.load_data()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8gI_A4_rAyL",
        "colab_type": "code",
        "outputId": "491c63c3-ea21-42ac-dd09-1f19d337899d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(f'samples_train shape : {samples_train.shape}')\n",
        "print(f'labels_train shape : {labels_train.shape}')\n",
        "print(f'samples_test shape : {samples_test.shape}')\n",
        "print(f'labels_test shape : {labels_test.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "samples_train shape : (60000, 28, 28)\n",
            "labels_train shape : (60000,)\n",
            "samples_test shape : (10000, 28, 28)\n",
            "labels_test shape : (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSShix5W8AYc",
        "colab_type": "text"
      },
      "source": [
        "Our training data, X_train, is in a 3D block. Using our (away, down,\n",
        "right) convention, it’s 60,000 slices deep, where each vertical slice is\n",
        "28 by 28 units.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-6.JPG?raw=1' width='800'/>\n",
        "\n",
        "The test data is set up the same way, except the stack is only 10,000\n",
        "images deep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu4cPn9u8VQL",
        "colab_type": "text"
      },
      "source": [
        "We’re going to reshape our data in the following sections, so let’s stash\n",
        "the original height and width of each image in a variable. We’ll also\n",
        "multiply them together and save that as the total number of pixels per\n",
        "image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMnpAww17xw1",
        "colab_type": "code",
        "outputId": "7d019111-31dc-47db-8d6e-6152aba9d6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "image_height = samples_train.shape[1]\n",
        "image_width = samples_train.shape[2]\n",
        "\n",
        "number_of_pixels = image_height * image_width\n",
        "number_of_pixels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVmP9eSU8tGJ",
        "colab_type": "text"
      },
      "source": [
        "The labels are given to us as one-dimensional lists. The training label\n",
        "list y_train has, as expected, a length of 60,000, since it’s providing\n",
        "one label for each sample in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79dEoLZf8lEO",
        "colab_type": "code",
        "outputId": "9ead1c38-78b2-45ef-e542-055b37df2988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'start of y_train: {labels_train[:15]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start of y_train: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEuCm5si9FXR",
        "colab_type": "text"
      },
      "source": [
        "So each entry in y_train is an integer. We expect it to be the label of\n",
        "the corresponding image in X_train. It always pays to check, so let’s\n",
        "look at the first 15 images in X_train.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-7.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpVh88O09V1o",
        "colab_type": "text"
      },
      "source": [
        "Now let’s look at the data itself. we print an arbitrary\n",
        "little rectangle from within the first image of X_train. A handy bit of\n",
        "Python to keep in mind is that by simply typing the name of a variable\n",
        "to the interpreter (rather than using a print statement), we sometimes\n",
        "get more information about the variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtRDZ2t583PI",
        "colab_type": "code",
        "outputId": "f2d5d508-6348-4abe-d1fc-0f88ed6487be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "samples_train[0, 5:12, 5:12]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0],\n",
              "       [  0,   0,   0,  30,  36,  94, 154],\n",
              "       [  0,   0,  49, 238, 253, 253, 253],\n",
              "       [  0,   0,  18, 219, 253, 253, 253],\n",
              "       [  0,   0,   0,  80, 156, 107, 253],\n",
              "       [  0,   0,   0,   0,  14,   1, 154],\n",
              "       [  0,   0,   0,   0,   0,   0, 139]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr_-RKYj9nIE",
        "colab_type": "text"
      },
      "source": [
        "As we might expect from grayscale image data, all of the values are between 0 and 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jDXSgO29gN1",
        "colab_type": "code",
        "outputId": "084c9e70-0698-4728-f537-39eac8c8d065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_train[:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIDkgVgU-K5j",
        "colab_type": "code",
        "outputId": "4311bee9-33af-43d4-ebe1-ace143a6f18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "samples_train[0, :, :]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHud0Oi8-Xvo",
        "colab_type": "code",
        "outputId": "dc1744be-44fa-490b-963a-00b2b6133841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_train[:1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1frkllgS-HJa",
        "colab_type": "text"
      },
      "source": [
        "To use this data for training with Keras, we need to turn the training\n",
        "and test sample data into normalized floating-point numbers, and turn\n",
        "the labels into one-hot encodings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlmY-6l4-klj",
        "colab_type": "text"
      },
      "source": [
        "## Train-test Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2qoz-DS-z3j",
        "colab_type": "text"
      },
      "source": [
        "Most data sets require us to manually split them into training and test\n",
        "sets. The MNIST data has already been split for us, but for completeness,\n",
        "let’s see how we’d do the job if we had to.\n",
        "\n",
        "The easiest and most common approach is to use scikit-learn’s\n",
        "train_test_split() function to do all the work for us. Suppose that\n",
        "the MNIST data came to us as only two tensors, called samples and\n",
        "labels, and we want to split it into a training set and a test set. A typical\n",
        "test set is often around 20% or 30% of the starting data, so let’s go\n",
        "down the middle with 25%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiEUqxky9_md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(samples, labels, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GI-3WPa_LM7",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/content-problems-8.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qHkZ-wb7uSw",
        "colab_type": "text"
      },
      "source": [
        "## Fixing the Data Type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErA9qbWH7xDZ",
        "colab_type": "text"
      },
      "source": [
        "In fact, Keras expects the specific type of floats that match its internal\n",
        "floatx parameter.\n",
        "\n",
        "Now that we know the format Keras expects for our floating-point\n",
        "numbers, we can return to our job of converting our samples into that\n",
        "form. The easy way to do this is to use the function cast_to_floatx()\n",
        "from the Keras backend, which takes a tensor as an argument and\n",
        "casts every element of that a tensor into the type specified by the current\n",
        "value of floatx."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75bmFrST_PAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = keras_backend.cast_to_floatx(samples_train)\n",
        "X_test = keras_backend.cast_to_floatx(labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNgNgzJ8L31",
        "colab_type": "text"
      },
      "source": [
        "## Normalizing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlF6DWfO8NKL",
        "colab_type": "text"
      },
      "source": [
        "The networks that we’ll be building to categorize the\n",
        "MNIST data will use convolution layers near the start, and those will\n",
        "work best with data that has been normalized so that each feature has\n",
        "been scaled to fit the range 0 to 1.\n",
        "\n",
        "Note that normalization is just for the features, and not the labels. The\n",
        "labels need to refer to the 10 different classes from 0 to 9, and we don’t\n",
        "want to change those values.\n",
        "\n",
        "Our feature data in X_train and X_test is originally made of integers in the range 0 to 255, This is a common range for a channel of image data. We’ve just converted these values to 32-bit floats, so we could say that they’re now in the range 0.0 to 255.0.\n",
        "\n",
        "We said above that we need to normalize our data to the range\n",
        "[0.0, 1.0] and  this helps to keep neuron\n",
        "outputs in the same range, which helps with regularization and delaying\n",
        "the onset of overfitting. And if we’re using an activation function\n",
        "like a sigmoid, it keeps our functions from saturating.\n",
        "\n",
        "We know that our pixels in the training and test data are in the range\n",
        "[0, 255]. All we want is to rescale all the pixels in the same way, compressing\n",
        "them from the range [0, 255] to the range [0,1]. Conceptually,\n",
        "this is like converting measurements in millimeters into kilometers, or\n",
        "vice-versa.\n",
        "\n",
        "We can scale our input data with Numpy’s interp() routine, which\n",
        "is designed for exactly this job. It takes an array (or tensor), an input\n",
        "range, and an output range. For each entry it will find its location in\n",
        "the first range (0 to 255) and find its corresponding position in the\n",
        "second range (0 to 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXubwAnX8Hsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.interp(X_train, [0, 255], [0, 1])\n",
        "X_test = np.interp(X_test, [0, 255], [0, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ87LNoi9baV",
        "colab_type": "text"
      },
      "source": [
        "This works perfectly, but since we know our data is in the range 0 to\n",
        "255, we can accomplish the same thing just by dividing all the pixels\n",
        "by 255.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNRzXFXa9aOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train /= 255.0\n",
        "X_test /= 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_25ft7su9yvV",
        "colab_type": "text"
      },
      "source": [
        "Let’s gather everything we’ve seen so far in one place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mWzn9R-9ns6",
        "colab_type": "code",
        "outputId": "f012aad5-fdc1-4eec-b1de-f5a098aa1599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras import backend as keras_backend\n",
        "\n",
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_height = X_train.shape[1]\n",
        "print(f'image_height = {image_height}')\n",
        "image_width = X_train.shape[2]\n",
        "print(f'image_width = {image_width}')\n",
        "number_of_pixels = image_height * image_width\n",
        "print(f'number_of_pixels = {number_of_pixels}')\n",
        "print()\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "print(f'Before scalling: \\n {X_train[:1]}')\n",
        "print()\n",
        "\n",
        "# scale data to range [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "print(f'After scalling: \\n {X_train[:1]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_height = 28\n",
            "image_width = 28\n",
            "number_of_pixels = 784\n",
            "\n",
            "Before scalling: \n",
            " [[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
            "    18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
            "   253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
            "   253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
            "   198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
            "    11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
            "     2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
            "    70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
            "   225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
            "   240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
            "   229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
            "   253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
            "   253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
            "    80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]]\n",
            "\n",
            "After scalling: \n",
            " [[[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336\n",
            "   0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            "   0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "   0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137\n",
            "   0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            "   0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.05490196 0.00392157 0.6039216\n",
            "   0.99215686 0.3529412  0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.54509807\n",
            "   0.99215686 0.74509805 0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.04313726\n",
            "   0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667\n",
            "   0.09803922 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            "   0.5882353  0.10588235 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.0627451  0.3647059  0.9882353\n",
            "   0.99215686 0.73333335 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.9764706\n",
            "   0.99215686 0.9764706  0.2509804  0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            "   0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686\n",
            "   0.98039216 0.7137255  0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.09411765 0.44705883\n",
            "   0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353\n",
            "   0.30588236 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.7647059  0.3137255  0.03529412 0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.21568628 0.6745098\n",
            "   0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
            "   0.52156866 0.04313726 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.53333336 0.99215686\n",
            "   0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xrzYX-_7cG",
        "colab_type": "text"
      },
      "source": [
        "Our training and test samples are now in floating-point format and\n",
        "scaled from 0.0 to 1.0.\n",
        "\n",
        "Now let’s pre-process the labels so that they’re ready for use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8lOHWU2AGrQ",
        "colab_type": "text"
      },
      "source": [
        "## Fixing the Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-dCPbzMAJCR",
        "colab_type": "text"
      },
      "source": [
        "We know that the MNIST data contains images of digits from 0 to 9.\n",
        "So in our network we’ll create an output layer with 10 neurons, one for\n",
        "each digit. Each neuron will produce a probability that the image it’s\n",
        "just been fed corresponds to that digit. The neuron with the highest\n",
        "value will be the network’s final prediction for the input.\n",
        "\n",
        "We’d like to compute an error value that tells us how close these 10\n",
        "values are to the values we want. To make this comparison easy, we\n",
        "represent the label for each image using one-hot encoding.\n",
        "\n",
        "In this case, it’s a list of 10 elements, where all are 0 except for 1 in slot 3.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/one-hot-encodings.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZgY_qIGDIat",
        "colab_type": "text"
      },
      "source": [
        "In this imaginary example, the network has given the value 3 the\n",
        "greatest probability, but it’s given each of the other digits some chance\n",
        "of being right, too. \n",
        "\n",
        "A perfect answer from the network would be a\n",
        "probability of 1 that the input is a 3, so all other choices would have\n",
        "a probability of 0. In other words, a perfect prediction would be the\n",
        "same as the label. The more the two are different, the higher the error.\n",
        "\n",
        "The one-hot form of the label simplifies this comparison of the output\n",
        "and the label.\n",
        "\n",
        "Turning each integer in a list into a one-hot encoding is such a common\n",
        "task that Keras provides a utility for it. The routine to_categorical()\n",
        "looks through an array of integers and finds the largest value, so it\n",
        "knows how many 0’s are needed to represent all the values that need\n",
        "to be encoded. It then makes a one-hot encoding for each integer in\n",
        "the list. The output of to_categorial() is a list of these encodings,\n",
        "which are themselves lists of 0’s and 1’s.\n",
        "\n",
        "Let’s see one-hot encoding in action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XBw-9WE_mjW",
        "colab_type": "code",
        "outputId": "0c7ce9a5-06fd-485b-e8de-cf807a057545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# print the first 5 entries of the original y_train array\n",
        "y_train[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siXDsu52EMJC",
        "colab_type": "code",
        "outputId": "3d8cc856-6636-40ca-b623-fe81c0fe2faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# encode the y_train array as one-hot lists\n",
        "y_train = to_categorical(y_train)\n",
        "# print the new first 5 entries of y_train, now one-hot encoded\n",
        "y_train[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq7-VK07Ehik",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the output is a 2D grid with one row for each input.\n",
        "Every entry is 0 except for a single 1, located at the index corresponding\n",
        "to the original y_train value for that row.\n",
        "\n",
        "We might be tempted to simply pass y_train and then y_test to\n",
        "to_categorical() in succession and move on, but that could introduce\n",
        "a subtle bug. The problem is that the largest value in one list\n",
        "might be different than the largest value in the other, giving us lists of\n",
        "different sizes.\n",
        "\n",
        "For instance, suppose that the test data was missing any images of\n",
        "the digit 9. That means that y_test will contain only the digits 0 to 8.\n",
        "When we use to_categorical() we’ll get back a list that has only 9\n",
        "items. This will cause trouble later when we want to compare it to the\n",
        "values in our output layer, which has a score for each of 10 categories.\n",
        "\n",
        "We don’t have to worry about this problem with the MNIST data,\n",
        "because it has examples for every image in both sets, but it might come\n",
        "up in other data sets.\n",
        "\n",
        "There’s an easy, general solution that will always avoid this problem.\n",
        "It involves using an optional argument to to_categorial() that overrides\n",
        "its scanning step. This argument, called num_classes, tells the\n",
        "routine to always make lists of the given length.\n",
        "\n",
        "The value of num_classes has to be at least big enough to encode all\n",
        "the possible values, or we’ll get an error. If num_classes is bigger than\n",
        "necessary, that’s fine, and the extra values at the end will always be 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgCKLoYFEXYD",
        "colab_type": "code",
        "outputId": "58658681-fbb8-4fd2-ca6f-2a339ec8147f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# combine the input lists to find largest value\n",
        "# in either list, then add 1 because the values start at 0\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test)).astype(np.int32)\n",
        "print(f'number_of_classes: {number_of_classes}')\n",
        "\n",
        "# encode each list into one-hot arrays of the size we just found\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number_of_classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXfebK-uGvVb",
        "colab_type": "text"
      },
      "source": [
        "Sometimes we want the original list of integers somewhere else in the\n",
        "program, when we do cross-validation. We can “undo”\n",
        "the one-hot encoding in two ways. If the one-hot encoding is represented\n",
        "as a regular Python list (that is, not a NumPy array), we can use\n",
        "Python’s built-in index() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5geVWiQFiBK",
        "colab_type": "code",
        "outputId": "4748dc62-4143-4f5d-aed2-5f275c8a60df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "one_hot = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "print(f'one-hot represents the integer {one_hot.index(1)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one-hot represents the integer 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiFQaqlHJFt",
        "colab_type": "text"
      },
      "source": [
        "If the one-hot version is a NumPy array, then we can’t use index(),\n",
        "because NumPy doesn’t support that method. There are several ways\n",
        "to use NumPy to find the index of a single 1 in list of 0’s.\n",
        "This uses NumPy’s argmax() method, which\n",
        "returns the index of the largest value in a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2GDUCppG_BV",
        "colab_type": "code",
        "outputId": "cacd95ad-f1ab-4ec4-c453-269248662566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "one_hot_np = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
        "print(f'one_hot_np represents the integer {np.argmax(one_hot_np)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "one_hot_np represents the integer 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpoeaw1-Hqwo",
        "colab_type": "text"
      },
      "source": [
        "Because one-hot encoding is so common, scikit-learn also offers a\n",
        "tool to perform it. It’s in the preprocessing module, and is called\n",
        "OneHotEncoder()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8NTXsHeHtds",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing All in One Place"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlqceZ5vHuYe",
        "colab_type": "text"
      },
      "source": [
        "To recap, we began by reading in (and possibly downloading) the\n",
        "MNIST data, and then prepared each image for Keras by changing it\n",
        "from integers to floats, and then normalized it. Then we created onehot\n",
        "encodings of our labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HihgfSJHa9Z",
        "colab_type": "code",
        "outputId": "2dae0608-809e-46d5-a84d-ab04ba4525a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras import backend as keras_backend\n",
        "\n",
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_height = X_train.shape[1]\n",
        "print(f'image_height = {image_height}')\n",
        "image_width = X_train.shape[2]\n",
        "print(f'image_width = {image_width}')\n",
        "number_of_pixels = image_height * image_width\n",
        "print(f'number_of_pixels = {number_of_pixels}')\n",
        "print()\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "print(f'Before scalling: \\n {X_train[:1]}')\n",
        "print()\n",
        "\n",
        "# scale data to range [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "print(f'After scalling: \\n {X_train[:1]}')\n",
        "print()\n",
        "\n",
        "# save the original y_train and y_test\n",
        "original_y_train = y_train\n",
        "original_y_test = y_test\n",
        "\n",
        "# replace label data with one-hot encoded versions\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test)).astype(np.int32)\n",
        "print(f'number_of_classes: {number_of_classes}')\n",
        "\n",
        "# encode each list into one-hot arrays of the size we just found\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_height = 28\n",
            "image_width = 28\n",
            "number_of_pixels = 784\n",
            "\n",
            "Before scalling: \n",
            " [[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
            "    18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
            "   253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
            "   253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
            "   198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
            "    11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
            "     2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
            "    70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
            "   225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
            "   240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "    46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
            "   229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
            "   253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
            "   253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
            "    80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "     0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]]\n",
            "\n",
            "After scalling: \n",
            " [[[0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336\n",
            "   0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            "   0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "   0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137\n",
            "   0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            "   0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.05490196 0.00392157 0.6039216\n",
            "   0.99215686 0.3529412  0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.54509807\n",
            "   0.99215686 0.74509805 0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.04313726\n",
            "   0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667\n",
            "   0.09803922 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            "   0.5882353  0.10588235 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.0627451  0.3647059  0.9882353\n",
            "   0.99215686 0.73333335 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.9764706\n",
            "   0.99215686 0.9764706  0.2509804  0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            "   0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686\n",
            "   0.98039216 0.7137255  0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.09411765 0.44705883\n",
            "   0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353\n",
            "   0.30588236 0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            "   0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686\n",
            "   0.99215686 0.7647059  0.3137255  0.03529412 0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.21568628 0.6745098\n",
            "   0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
            "   0.52156866 0.04313726 0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.53333336 0.99215686\n",
            "   0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]\n",
            "  [0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.         0.         0.\n",
            "   0.         0.         0.         0.        ]]]\n",
            "\n",
            "number_of_classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}