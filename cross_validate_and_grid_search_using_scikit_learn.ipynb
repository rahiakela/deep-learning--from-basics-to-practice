{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-validate-and-grid-search-using-scikit-learn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/cross_validate_and_grid_search_using_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJaJDh8FPaCa",
        "colab_type": "text"
      },
      "source": [
        "# Cross-validate and Grid-search using Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUV7FVUPkuh",
        "colab_type": "text"
      },
      "source": [
        "So far we’ve been searching our hyperparameters by hand. It’s been\n",
        "illuminating, but it also required a lot of manual effort.\n",
        "\n",
        "The scikit-learn library offers us routines to\n",
        "cross-validate our model (to estimate how good it is), and grid-search\n",
        "its hyperparameters (to find the best-performing combination).\n",
        "\n",
        "Keras doesn’t offer either of these tools directly, because it offers a way\n",
        "to use the ones already in scikit-learn.\n",
        "\n",
        "A popular approach is to extract a tiny piece of the data set, carefully\n",
        "selected to be representative of the whole, and search on that. Then\n",
        "each training run will be much faster.\n",
        "\n",
        "By cross-validating and grid-searching one or more of these little proxy\n",
        "databases, we can get some guidance for what models and hyperparameters\n",
        "are worth exploring on a larger scale. Then we can take that\n",
        "knowledge and work with larger and larger pieces of the dataset, tuning\n",
        "the hyperparameters at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFVvmT61QD2_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbR1F-52QFGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAKka4YRXwbo",
        "colab_type": "text"
      },
      "source": [
        "## Load and process the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYCe3VWFXxKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "086cd6f7-39af-444e-9be1-2277422e2f29"
      },
      "source": [
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_height = X_train.shape[1]\n",
        "image_width = X_train.shape[2]\n",
        "number_of_pixels = image_height * image_width\n",
        "\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "\n",
        "\n",
        "# scale data to range [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "\n",
        "# save the original y_train and y_test\n",
        "original_y_train = y_train\n",
        "original_y_test = y_test\n",
        "\n",
        "# replace label data with one-hot encoded versions\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test)).astype(np.int32)\n",
        "\n",
        "# encode each list into one-hot arrays of the size we just found\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)\n",
        "\n",
        "# reshape samples to 2D grid, one line per image\n",
        "X_train = X_train.reshape([X_train.shape[0], number_of_pixels])\n",
        "X_test = X_test.reshape([X_test.shape[0], number_of_pixels])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9kF53jaQFZs",
        "colab_type": "text"
      },
      "source": [
        "## Keras Wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLnsyk6tQGtI",
        "colab_type": "text"
      },
      "source": [
        "It would be nice to use scikit-learn’s cross-validation and grid-search\n",
        "tools directly on our Keras models. But Keras is a library that sits “on\n",
        "top” of scikit-learn. This means that scikit-learn doesn’t know anything\n",
        "about Keras and its models. But it also means that Keras knows\n",
        "everything about scikit-learn.\n",
        "\n",
        "Let us place a Keras model into scikit-learn,\n",
        "and then do cross-validation, grid search, or any other operation we\n",
        "like. From scikit-learn’s perspective, this object is just some custom\n",
        "estimator that we wrote and gave to it. It doesn’t know that there’s a\n",
        "deep network hiding inside.\n",
        "\n",
        "We pull off this trick by embedding our Keras model in an object of\n",
        "type KerasClassifier or KerasRegressor, depending on the job it\n",
        "does. These objects are called wrappers, since they “wrap” our Keras\n",
        "model in a disguise that makes it look and act like a scikit-learn estimator.\n",
        "\n",
        "Since both wrappers work identically, we’ll choose KerasClassifier\n",
        "as an example so we can stick with the MNIST classifiers we’ve been\n",
        "discussing so far.\n",
        "\n",
        "Let’s dig in, starting with the model-making function.\n",
        "\n",
        "This argument is named build_fn, short for “build function.” Its value\n",
        "is a function that we’ve written which will construct, compile, and\n",
        "return a Keras model.\n",
        "\n",
        "This model making-function will be called automatically\n",
        "by scikit-learn when the model is required. When we’re grid\n",
        "searching, the model will usually be built over and over again at the\n",
        "start of each new step of the search.\n",
        "\n",
        "So we have our model-making function that takes arguments, and a\n",
        "wrapper, and scikit-learn which is going to call our function. How do\n",
        "we get scikit-learn to include the arguments we want when it calls the\n",
        "model-making function?\n",
        "\n",
        "Happily, the mechanism is easy. The trick is in the naming of our\n",
        "arguments. when we create a search using\n",
        "scikit-learn, we provide it with a dictionary that names each parameter\n",
        "we want it to search on as a key, with values to be tried as the value.\n",
        "\n",
        "To see this in action, let’s start with a model-making function that\n",
        "takes parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULpO2HgZNX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a network of any number of (dense+ dropout) layers of the given size\n",
        "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
        "  model = Sequential()\n",
        "\n",
        "  # first layer is special, because it sets input_shape\n",
        "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=[number_of_pixels], kernel_constraint=max_norm(3)))\n",
        "  model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # now add in all the rest of the dense-dropout layers\n",
        "  for layer in range(number_of_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint=max_norm(3)))\n",
        "    model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # finish up with a softmax layer with 10 outputs\n",
        "  model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "  # compile the model and return it\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al1PvD12hioY",
        "colab_type": "text"
      },
      "source": [
        "## Wrap up make_model() function in KerasClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj2qN-QohoL_",
        "colab_type": "text"
      },
      "source": [
        "Wrap up our model-making function into a KerasClassifier, which\n",
        "will make it behave like a standard scikit-learn estimator.  We'll\n",
        "give all the arguments defaults which we can override later when\n",
        "we build the model as part of cross-validation or grid search.\n",
        "\n",
        "For instance, we will provide a value to number_of_layers, so this\n",
        "value will be passed to number_of_layers when make_model() is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFvX4fx8hFlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tl4Rn_Zi1pM",
        "colab_type": "text"
      },
      "source": [
        "In effect, the wrapper just takes the values we provide to it and passes\n",
        "them to the model-making function arguments of the same name. The\n",
        "syntax is a little confusing because it looks like KerasClassifier is\n",
        "taking these arguments for itself, but this is a funky bit of Python that\n",
        "doesn’t follow the common rules.\n",
        "\n",
        "If we hand kc_model to scikit-learn for cross-validation, make_model()\n",
        "will be called and passed the value 2 for number_of_layers, the value\n",
        "32 for the argument neurons_per_layer, and the string ′adam′ for\n",
        "optimizer.\n",
        "\n",
        "The last set of three arguments to KerasClassifier() (epochs,\n",
        "batch_size, and verbose) are not for our model, but are intended for\n",
        "scikit-learn. They get passed to the cross-validator’s fit() routine to\n",
        "control the training process.\n",
        "\n",
        "The key thing to remember is that the wrapper is basically remembering\n",
        "what values should be used for the arguments in the model-making\n",
        "function, and it will use those by default. It also remembers a few values\n",
        "that get passed on to scikit-learn. As long as the names we’re assigning\n",
        "to in the wrapper match the names in the model-making routine,\n",
        "everything will be automatically matched up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMs1vjVjjXqo",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZYptJDja-Z",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBySlZxi04U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}