{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-validate-and-grid-search-using-scikit-learn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/cross_validate_and_grid_search_using_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJaJDh8FPaCa",
        "colab_type": "text"
      },
      "source": [
        "# Cross-validate and Grid-search using Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUV7FVUPkuh",
        "colab_type": "text"
      },
      "source": [
        "So far we’ve been searching our hyperparameters by hand. It’s been\n",
        "illuminating, but it also required a lot of manual effort.\n",
        "\n",
        "The scikit-learn library offers us routines to\n",
        "cross-validate our model (to estimate how good it is), and grid-search\n",
        "its hyperparameters (to find the best-performing combination).\n",
        "\n",
        "Keras doesn’t offer either of these tools directly, because it offers a way\n",
        "to use the ones already in scikit-learn.\n",
        "\n",
        "A popular approach is to extract a tiny piece of the data set, carefully\n",
        "selected to be representative of the whole, and search on that. Then\n",
        "each training run will be much faster.\n",
        "\n",
        "By cross-validating and grid-searching one or more of these little proxy\n",
        "databases, we can get some guidance for what models and hyperparameters\n",
        "are worth exploring on a larger scale. Then we can take that\n",
        "knowledge and work with larger and larger pieces of the dataset, tuning\n",
        "the hyperparameters at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFVvmT61QD2_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbR1F-52QFGH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62215b99-51b9-4b86-e1fa-90158de5c7d0"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAKka4YRXwbo",
        "colab_type": "text"
      },
      "source": [
        "## Load and process the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYCe3VWFXxKF",
        "colab_type": "code",
        "outputId": "c42578be-d29d-42c1-8fb0-b2791404a926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_height = X_train.shape[1]\n",
        "image_width = X_train.shape[2]\n",
        "number_of_pixels = image_height * image_width\n",
        "\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "\n",
        "\n",
        "# scale data to range [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "\n",
        "# save the original y_train and y_test\n",
        "original_y_train = y_train\n",
        "original_y_test = y_test\n",
        "\n",
        "# replace label data with one-hot encoded versions\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test)).astype(np.int32)\n",
        "\n",
        "# encode each list into one-hot arrays of the size we just found\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)\n",
        "\n",
        "# reshape samples to 2D grid, one line per image\n",
        "X_train = X_train.reshape([X_train.shape[0], number_of_pixels])\n",
        "X_test = X_test.reshape([X_test.shape[0], number_of_pixels])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9kF53jaQFZs",
        "colab_type": "text"
      },
      "source": [
        "## Keras Wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLnsyk6tQGtI",
        "colab_type": "text"
      },
      "source": [
        "It would be nice to use scikit-learn’s cross-validation and grid-search\n",
        "tools directly on our Keras models. But Keras is a library that sits “on\n",
        "top” of scikit-learn. This means that scikit-learn doesn’t know anything\n",
        "about Keras and its models. But it also means that Keras knows\n",
        "everything about scikit-learn.\n",
        "\n",
        "Let us place a Keras model into scikit-learn,\n",
        "and then do cross-validation, grid search, or any other operation we\n",
        "like. From scikit-learn’s perspective, this object is just some custom\n",
        "estimator that we wrote and gave to it. It doesn’t know that there’s a\n",
        "deep network hiding inside.\n",
        "\n",
        "We pull off this trick by embedding our Keras model in an object of\n",
        "type KerasClassifier or KerasRegressor, depending on the job it\n",
        "does. These objects are called wrappers, since they “wrap” our Keras\n",
        "model in a disguise that makes it look and act like a scikit-learn estimator.\n",
        "\n",
        "Since both wrappers work identically, we’ll choose KerasClassifier\n",
        "as an example so we can stick with the MNIST classifiers we’ve been\n",
        "discussing so far.\n",
        "\n",
        "Let’s dig in, starting with the model-making function.\n",
        "\n",
        "This argument is named build_fn, short for “build function.” Its value\n",
        "is a function that we’ve written which will construct, compile, and\n",
        "return a Keras model.\n",
        "\n",
        "This model making-function will be called automatically\n",
        "by scikit-learn when the model is required. When we’re grid\n",
        "searching, the model will usually be built over and over again at the\n",
        "start of each new step of the search.\n",
        "\n",
        "So we have our model-making function that takes arguments, and a\n",
        "wrapper, and scikit-learn which is going to call our function. How do\n",
        "we get scikit-learn to include the arguments we want when it calls the\n",
        "model-making function?\n",
        "\n",
        "Happily, the mechanism is easy. The trick is in the naming of our\n",
        "arguments. when we create a search using\n",
        "scikit-learn, we provide it with a dictionary that names each parameter\n",
        "we want it to search on as a key, with values to be tried as the value.\n",
        "\n",
        "To see this in action, let’s start with a model-making function that\n",
        "takes parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULpO2HgZNX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a network of any number of (dense+ dropout) layers of the given size\n",
        "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
        "  model = Sequential()\n",
        "\n",
        "  # first layer is special, because it sets input_shape\n",
        "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=[number_of_pixels], kernel_constraint=max_norm(3)))\n",
        "  model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # now add in all the rest of the dense-dropout layers\n",
        "  for layer in range(number_of_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint=max_norm(3)))\n",
        "    model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # finish up with a softmax layer with 10 outputs\n",
        "  model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "  # compile the model and return it\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al1PvD12hioY",
        "colab_type": "text"
      },
      "source": [
        "## Wrap up make_model() function in KerasClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj2qN-QohoL_",
        "colab_type": "text"
      },
      "source": [
        "Wrap up our model-making function into a KerasClassifier, which\n",
        "will make it behave like a standard scikit-learn estimator.  We'll\n",
        "give all the arguments defaults which we can override later when\n",
        "we build the model as part of cross-validation or grid search.\n",
        "\n",
        "For instance, we will provide a value to number_of_layers, so this\n",
        "value will be passed to number_of_layers when make_model() is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFvX4fx8hFlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tl4Rn_Zi1pM",
        "colab_type": "text"
      },
      "source": [
        "In effect, the wrapper just takes the values we provide to it and passes\n",
        "them to the model-making function arguments of the same name. The\n",
        "syntax is a little confusing because it looks like KerasClassifier is\n",
        "taking these arguments for itself, but this is a funky bit of Python that\n",
        "doesn’t follow the common rules.\n",
        "\n",
        "If we hand kc_model to scikit-learn for cross-validation, make_model()\n",
        "will be called and passed the value 2 for number_of_layers, the value\n",
        "32 for the argument neurons_per_layer, and the string ′adam′ for\n",
        "optimizer.\n",
        "\n",
        "The last set of three arguments to KerasClassifier() (epochs,\n",
        "batch_size, and verbose) are not for our model, but are intended for\n",
        "scikit-learn. They get passed to the cross-validator’s fit() routine to\n",
        "control the training process.\n",
        "\n",
        "The key thing to remember is that the wrapper is basically remembering\n",
        "what values should be used for the arguments in the model-making\n",
        "function, and it will use those by default. It also remembers a few values\n",
        "that get passed on to scikit-learn. As long as the names we’re assigning\n",
        "to in the wrapper match the names in the model-making routine,\n",
        "everything will be automatically matched up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMs1vjVjjXqo",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZYptJDja-Z",
        "colab_type": "text"
      },
      "source": [
        "Applying cross-validation may seem pointless. After all, we already\n",
        "have an excellent, large testing set. What more are we going to learn\n",
        "from cross-validation that we haven’t already seen by using our validation\n",
        "data?\n",
        "\n",
        "In this case, not much. We should expect the results of cross-validation\n",
        "to be very close to what we saw above.\n",
        "\n",
        "Another challenge of validation sets comes when we’re working with\n",
        "a small version of an original dataset. If this dataset is small,cross-validation is a great way to evaluate it without making the training set even smaller by making a dedicated validation set.\n",
        "\n",
        "Cross-validation requires training and then validating our entire\n",
        "model over and over again with slightly different data. We’ll be using\n",
        "10 folds, so each session of the cross-validator will take 10 times longer.\n",
        "\n",
        "One issue is that scikit-learn’s cross-validation function\n",
        "cross_val_score() doesn’t want the one-hot encoded version of our\n",
        "label data. It wants the original versions that contain lists of integers.\n",
        "\n",
        "The other issue has to do with what data we pass to the cross-validation\n",
        "system. As we’ve done before, we’ll simply pretend that we don’t\n",
        "have a validation set, and treat the training data as if it was our entire\n",
        "dataset. We’ll let the cross-validator manage the train-validation split\n",
        "for us.\n",
        "\n",
        "Now let’s get this cross-validation going. There are two tasks to perform.\n",
        "* First, we’ll make the object that drives the cross-validation\n",
        "process. \n",
        "* Then use StratifiedKFold() with 10 splits. \n",
        "\n",
        "We’ll shuffle the data, and we’ll set the optional\n",
        "random_state variable to the value of random_seed that we already\n",
        "have around. That’s useful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdMfI5BicdEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxVxxLlKelIB",
        "colab_type": "text"
      },
      "source": [
        "We just tell scikit-learn to run the cross-validator and track\n",
        "the scores by calling cross_val_score() with our model, our training\n",
        "data and original labels, and our folding object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBySlZxi04U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "results = cross_val_score(kc_model, X_train, original_y_train, cv=kfold, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ubXCU5iicsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "880e911b-670c-47a4-f1f1-1dae1b62a2da"
      },
      "source": [
        "print(f'results = {str(results)} \\n results.mean = {str(results.mean())}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results = [0.95820844 0.95968682 0.96250623 0.96083331 0.96633333 0.9666611\n",
            " 0.9599933  0.9613269  0.96815073 0.96114075] \n",
            " results.mean = 0.9624840915203094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP5BYhOSfOJq",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cehqy47fe74s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Build a network of any number of (dense+ dropout) layers of the given size\n",
        "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
        "  model = Sequential()\n",
        "\n",
        "  # first layer is special, because it sets input_shape\n",
        "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=[number_of_pixels], kernel_constraint=max_norm(3)))\n",
        "  model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # now add in all the rest of the dense-dropout layers\n",
        "  for layer in range(number_of_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint=max_norm(3)))\n",
        "    model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # finish up with a softmax layer with 10 outputs\n",
        "  model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "  # compile the model and return it\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# make the model and wrap it up for scikit-learn\n",
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )\n",
        "\n",
        "# create cross-validator\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
        "results = cross_val_score(kc_model, X_train, original_y_train, cv=kfold, verbose=0)\n",
        "print(f'results = {str(results)} \\n results.mean = {str(results.mean())}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrQYoWXnggXM",
        "colab_type": "text"
      },
      "source": [
        "So the cross-validation run is telling us that on the original dataset of\n",
        "60,000 images, we got a performance of a bit more than 95% accuracy.\n",
        "That’s a just about the same as what we saw graphically for this\n",
        "model way back in Figure 24.13, where the validation accuracy was\n",
        "just a smidge better than 96%.\n",
        "\n",
        "That’s reassuring. It says that this whole wrapping and cross-validating\n",
        "scheme is producing the same results that we got when we trained\n",
        "and tested the model ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDLnL5k2gzvL",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation with Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88_lRxsg2cW",
        "colab_type": "text"
      },
      "source": [
        "Because we already normalized the training data to the range [0,1] when we divided it by 255. So when cross-validation grabs a random 90% of these samples and trains on them, it’s likely to get samples that run from 0 to 1.\n",
        "\n",
        "In general, the data that’s going to get chosen from our database and used for cross-validation won’t be normalized to the range [0,1]. It’s up to us to get that normalization in there, and then apply that same transform to the part of the data that was set aside for testing in that run.\n",
        "\n",
        "We can normalize the particular piece of training\n",
        "data that’s built for each pass through cross-validation by building\n",
        "a Pipeline object composed of two steps: a normalizer followed by\n",
        "our model.\n",
        "\n",
        "Let’s do this by first making our objects, and then assembling them\n",
        "into a Pipeline object. For demonstration purposes our pipeline will\n",
        "contain a MinMaxScaler from scikit-learn, followed by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq_QNiuGiwD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('normalize_step', MinMaxScaler()))\n",
        "estimators.append(('model_step', kc_model))\n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TRAFBFUmaeo",
        "colab_type": "text"
      },
      "source": [
        "Constructing a pipeline this way is useful when we want to later refer\n",
        "to the individual steps. We’ll need to do that soon when we use grid\n",
        "searching.\n",
        "\n",
        "But for this cross-validation step, we don’t need that kind of access.\n",
        "We’ll often see code that builds the pipeline in one line, using the shortcut\n",
        "make_pipeline() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq9THq0QmUXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = make_pipeline(MinMaxScaler(), kc_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lt0rTQAnW1w",
        "colab_type": "text"
      },
      "source": [
        "These two pipeline objects are the same. The only difference is that\n",
        "we’ve given our own names to the steps in the first version.\n",
        "\n",
        "To use our pipeline object, we just give it to cross_val_score() in\n",
        "place of a model (or wrapped model). Scikit-learn will recognize that\n",
        "it’s a pipeline and take care of all the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3iE50I1ng8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = cross_val_score(pipeline, X_train, original_y_train, cv=kfold, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M7LnhJTolVj",
        "colab_type": "text"
      },
      "source": [
        "Putting these new lines together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFUwKirKomMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9d0a53f6-df25-4439-f152-077bbf5f29de"
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# make the model and wrap it up for scikit-learn\n",
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )\n",
        "\n",
        "# create pipeline\n",
        "pipeline = make_pipeline(MinMaxScaler(), kc_model)\n",
        "\n",
        "# create cross-validator\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
        "\n",
        "# execute cross validation using pipeline and k-fold cross-validator\n",
        "results2 = cross_val_score(pipeline, X_train, original_y_train, cv=kfold, verbose=0)\n",
        "print(f'results = {str(results2)} \\n results.mean = {str(results2.mean())}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results = [0.96120548 0.96002001 0.96400601 0.96216667 0.96216667 0.96682781\n",
            " 0.96266043 0.96449411 0.96631652 0.95980656] \n",
            " results.mean = 0.9629670262336731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuM3HIGpyXy",
        "colab_type": "text"
      },
      "source": [
        "Cross-validation is a great way to get a handle on the quality of our\n",
        "model. It’s not so great when training times start to push our patience,\n",
        "since every fold is essentially a brand-new full-length training and\n",
        "testing process. Using 10 folds requires training and then testing our\n",
        "model 10 times in a row."
      ]
    }
  ]
}