{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-validate-and-grid-search-using-scikit-learn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/cross_validate_and_grid_search_using_scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJaJDh8FPaCa",
        "colab_type": "text"
      },
      "source": [
        "# Cross-validate and Grid-search using Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtUV7FVUPkuh",
        "colab_type": "text"
      },
      "source": [
        "So far we’ve been searching our hyperparameters by hand. It’s been\n",
        "illuminating, but it also required a lot of manual effort.\n",
        "\n",
        "The scikit-learn library offers us routines to\n",
        "cross-validate our model (to estimate how good it is), and grid-search\n",
        "its hyperparameters (to find the best-performing combination).\n",
        "\n",
        "Keras doesn’t offer either of these tools directly, because it offers a way\n",
        "to use the ones already in scikit-learn.\n",
        "\n",
        "A popular approach is to extract a tiny piece of the data set, carefully\n",
        "selected to be representative of the whole, and search on that. Then\n",
        "each training run will be much faster.\n",
        "\n",
        "By cross-validating and grid-searching one or more of these little proxy\n",
        "databases, we can get some guidance for what models and hyperparameters\n",
        "are worth exploring on a larger scale. Then we can take that\n",
        "knowledge and work with larger and larger pieces of the dataset, tuning\n",
        "the hyperparameters at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFVvmT61QD2_",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbR1F-52QFGH",
        "colab_type": "code",
        "outputId": "90ef2862-9f78-4111-9bbb-7a24e57d30c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAKka4YRXwbo",
        "colab_type": "text"
      },
      "source": [
        "## Load and process the MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYCe3VWFXxKF",
        "colab_type": "code",
        "outputId": "586d7f7b-e618-499f-96d2-8291e6bc3d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_height = X_train.shape[1]\n",
        "image_width = X_train.shape[2]\n",
        "number_of_pixels = image_height * image_width\n",
        "\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "\n",
        "\n",
        "# scale data to range [0, 1]\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "\n",
        "# save the original y_train and y_test\n",
        "original_y_train = y_train\n",
        "original_y_test = y_test\n",
        "\n",
        "# replace label data with one-hot encoded versions\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test)).astype(np.int32)\n",
        "\n",
        "# encode each list into one-hot arrays of the size we just found\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)\n",
        "\n",
        "# reshape samples to 2D grid, one line per image\n",
        "X_train = X_train.reshape([X_train.shape[0], number_of_pixels])\n",
        "X_test = X_test.reshape([X_test.shape[0], number_of_pixels])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9kF53jaQFZs",
        "colab_type": "text"
      },
      "source": [
        "## Keras Wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLnsyk6tQGtI",
        "colab_type": "text"
      },
      "source": [
        "It would be nice to use scikit-learn’s cross-validation and grid-search\n",
        "tools directly on our Keras models. But Keras is a library that sits “on\n",
        "top” of scikit-learn. This means that scikit-learn doesn’t know anything\n",
        "about Keras and its models. But it also means that Keras knows\n",
        "everything about scikit-learn.\n",
        "\n",
        "Let us place a Keras model into scikit-learn,\n",
        "and then do cross-validation, grid search, or any other operation we\n",
        "like. From scikit-learn’s perspective, this object is just some custom\n",
        "estimator that we wrote and gave to it. It doesn’t know that there’s a\n",
        "deep network hiding inside.\n",
        "\n",
        "We pull off this trick by embedding our Keras model in an object of\n",
        "type KerasClassifier or KerasRegressor, depending on the job it\n",
        "does. These objects are called wrappers, since they “wrap” our Keras\n",
        "model in a disguise that makes it look and act like a scikit-learn estimator.\n",
        "\n",
        "Since both wrappers work identically, we’ll choose KerasClassifier\n",
        "as an example so we can stick with the MNIST classifiers we’ve been\n",
        "discussing so far.\n",
        "\n",
        "Let’s dig in, starting with the model-making function.\n",
        "\n",
        "This argument is named build_fn, short for “build function.” Its value\n",
        "is a function that we’ve written which will construct, compile, and\n",
        "return a Keras model.\n",
        "\n",
        "This model making-function will be called automatically\n",
        "by scikit-learn when the model is required. When we’re grid\n",
        "searching, the model will usually be built over and over again at the\n",
        "start of each new step of the search.\n",
        "\n",
        "So we have our model-making function that takes arguments, and a\n",
        "wrapper, and scikit-learn which is going to call our function. How do\n",
        "we get scikit-learn to include the arguments we want when it calls the\n",
        "model-making function?\n",
        "\n",
        "Happily, the mechanism is easy. The trick is in the naming of our\n",
        "arguments. when we create a search using\n",
        "scikit-learn, we provide it with a dictionary that names each parameter\n",
        "we want it to search on as a key, with values to be tried as the value.\n",
        "\n",
        "To see this in action, let’s start with a model-making function that\n",
        "takes parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULpO2HgZNX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a network of any number of (dense+ dropout) layers of the given size\n",
        "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
        "  model = Sequential()\n",
        "\n",
        "  # first layer is special, because it sets input_shape\n",
        "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=[number_of_pixels], kernel_constraint=max_norm(3)))\n",
        "  model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # now add in all the rest of the dense-dropout layers\n",
        "  for layer in range(number_of_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint=max_norm(3)))\n",
        "    model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # finish up with a softmax layer with 10 outputs\n",
        "  model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "  # compile the model and return it\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al1PvD12hioY",
        "colab_type": "text"
      },
      "source": [
        "## Wrap up make_model() function in KerasClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj2qN-QohoL_",
        "colab_type": "text"
      },
      "source": [
        "Wrap up our model-making function into a KerasClassifier, which\n",
        "will make it behave like a standard scikit-learn estimator.  We'll\n",
        "give all the arguments defaults which we can override later when\n",
        "we build the model as part of cross-validation or grid search.\n",
        "\n",
        "For instance, we will provide a value to number_of_layers, so this\n",
        "value will be passed to number_of_layers when make_model() is called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFvX4fx8hFlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tl4Rn_Zi1pM",
        "colab_type": "text"
      },
      "source": [
        "In effect, the wrapper just takes the values we provide to it and passes\n",
        "them to the model-making function arguments of the same name. The\n",
        "syntax is a little confusing because it looks like KerasClassifier is\n",
        "taking these arguments for itself, but this is a funky bit of Python that\n",
        "doesn’t follow the common rules.\n",
        "\n",
        "If we hand kc_model to scikit-learn for cross-validation, make_model()\n",
        "will be called and passed the value 2 for number_of_layers, the value\n",
        "32 for the argument neurons_per_layer, and the string ′adam′ for\n",
        "optimizer.\n",
        "\n",
        "The last set of three arguments to KerasClassifier() (epochs,\n",
        "batch_size, and verbose) are not for our model, but are intended for\n",
        "scikit-learn. They get passed to the cross-validator’s fit() routine to\n",
        "control the training process.\n",
        "\n",
        "The key thing to remember is that the wrapper is basically remembering\n",
        "what values should be used for the arguments in the model-making\n",
        "function, and it will use those by default. It also remembers a few values\n",
        "that get passed on to scikit-learn. As long as the names we’re assigning\n",
        "to in the wrapper match the names in the model-making routine,\n",
        "everything will be automatically matched up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMs1vjVjjXqo",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZYptJDja-Z",
        "colab_type": "text"
      },
      "source": [
        "Applying cross-validation may seem pointless. After all, we already\n",
        "have an excellent, large testing set. What more are we going to learn\n",
        "from cross-validation that we haven’t already seen by using our validation\n",
        "data?\n",
        "\n",
        "In this case, not much. We should expect the results of cross-validation\n",
        "to be very close to what we saw above.\n",
        "\n",
        "Another challenge of validation sets comes when we’re working with\n",
        "a small version of an original dataset. If this dataset is small,cross-validation is a great way to evaluate it without making the training set even smaller by making a dedicated validation set.\n",
        "\n",
        "Cross-validation requires training and then validating our entire\n",
        "model over and over again with slightly different data. We’ll be using\n",
        "10 folds, so each session of the cross-validator will take 10 times longer.\n",
        "\n",
        "One issue is that scikit-learn’s cross-validation function\n",
        "cross_val_score() doesn’t want the one-hot encoded version of our\n",
        "label data. It wants the original versions that contain lists of integers.\n",
        "\n",
        "The other issue has to do with what data we pass to the cross-validation\n",
        "system. As we’ve done before, we’ll simply pretend that we don’t\n",
        "have a validation set, and treat the training data as if it was our entire\n",
        "dataset. We’ll let the cross-validator manage the train-validation split\n",
        "for us.\n",
        "\n",
        "Now let’s get this cross-validation going. There are two tasks to perform.\n",
        "* First, we’ll make the object that drives the cross-validation\n",
        "process. \n",
        "* Then use StratifiedKFold() with 10 splits. \n",
        "\n",
        "We’ll shuffle the data, and we’ll set the optional\n",
        "random_state variable to the value of random_seed that we already\n",
        "have around. That’s useful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdMfI5BicdEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxVxxLlKelIB",
        "colab_type": "text"
      },
      "source": [
        "We just tell scikit-learn to run the cross-validator and track\n",
        "the scores by calling cross_val_score() with our model, our training\n",
        "data and original labels, and our folding object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBySlZxi04U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "results = cross_val_score(kc_model, X_train, original_y_train, cv=kfold, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ubXCU5iicsV",
        "colab_type": "code",
        "outputId": "993f71fc-0f46-40be-925f-b03c79278654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(f'results = {str(results)} \\n results.mean = {str(results.mean())}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results = [0.96203798 0.96235216 0.96450591 0.96249998 0.96316665 0.96632773\n",
            " 0.96332723 0.96549428 0.968651   0.96397597] \n",
            " results.mean = 0.9642338871955871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP5BYhOSfOJq",
        "colab_type": "text"
      },
      "source": [
        "Putting it all together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cehqy47fe74s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "# Build a network of any number of (dense+ dropout) layers of the given size\n",
        "def make_model(number_of_layers=2, neurons_per_layer=32, dropout_ratio=0.2, optimizer='adam'):\n",
        "  model = Sequential()\n",
        "\n",
        "  # first layer is special, because it sets input_shape\n",
        "  model.add(Dense(neurons_per_layer, activation='relu', input_shape=[number_of_pixels], kernel_constraint=max_norm(3)))\n",
        "  model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # now add in all the rest of the dense-dropout layers\n",
        "  for layer in range(number_of_layers - 1):\n",
        "    model.add(Dense(neurons_per_layer, activation='relu', kernel_constraint=max_norm(3)))\n",
        "    model.add(Dropout(dropout_ratio))\n",
        "\n",
        "  # finish up with a softmax layer with 10 outputs\n",
        "  model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "  # compile the model and return it\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "# make the model and wrap it up for scikit-learn\n",
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )\n",
        "\n",
        "# create cross-validator\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
        "results = cross_val_score(kc_model, X_train, original_y_train, cv=kfold, verbose=0)\n",
        "print(f'results = {str(results)} \\n results.mean = {str(results.mean())}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrQYoWXnggXM",
        "colab_type": "text"
      },
      "source": [
        "So the cross-validation run is telling us that on the original dataset of\n",
        "60,000 images, we got a performance of a bit more than 95% accuracy.\n",
        "That’s a just about the same as what we saw graphically for this\n",
        "model way back in Figure 24.13, where the validation accuracy was\n",
        "just a smidge better than 96%.\n",
        "\n",
        "That’s reassuring. It says that this whole wrapping and cross-validating\n",
        "scheme is producing the same results that we got when we trained\n",
        "and tested the model ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDLnL5k2gzvL",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation with Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88_lRxsg2cW",
        "colab_type": "text"
      },
      "source": [
        "Because we already normalized the training data to the range [0,1] when we divided it by 255. So when cross-validation grabs a random 90% of these samples and trains on them, it’s likely to get samples that run from 0 to 1.\n",
        "\n",
        "In general, the data that’s going to get chosen from our database and used for cross-validation won’t be normalized to the range [0,1]. It’s up to us to get that normalization in there, and then apply that same transform to the part of the data that was set aside for testing in that run.\n",
        "\n",
        "We can normalize the particular piece of training\n",
        "data that’s built for each pass through cross-validation by building\n",
        "a Pipeline object composed of two steps: a normalizer followed by\n",
        "our model.\n",
        "\n",
        "Let’s do this by first making our objects, and then assembling them\n",
        "into a Pipeline object. For demonstration purposes our pipeline will\n",
        "contain a MinMaxScaler from scikit-learn, followed by our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq_QNiuGiwD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('normalize_step', MinMaxScaler()))\n",
        "estimators.append(('model_step', kc_model))\n",
        "pipeline = Pipeline(estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TRAFBFUmaeo",
        "colab_type": "text"
      },
      "source": [
        "Constructing a pipeline this way is useful when we want to later refer\n",
        "to the individual steps. We’ll need to do that soon when we use grid\n",
        "searching.\n",
        "\n",
        "But for this cross-validation step, we don’t need that kind of access.\n",
        "We’ll often see code that builds the pipeline in one line, using the shortcut\n",
        "make_pipeline() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq9THq0QmUXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = make_pipeline(MinMaxScaler(), kc_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lt0rTQAnW1w",
        "colab_type": "text"
      },
      "source": [
        "These two pipeline objects are the same. The only difference is that\n",
        "we’ve given our own names to the steps in the first version.\n",
        "\n",
        "To use our pipeline object, we just give it to cross_val_score() in\n",
        "place of a model (or wrapped model). Scikit-learn will recognize that\n",
        "it’s a pipeline and take care of all the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3iE50I1ng8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = cross_val_score(pipeline, X_train, original_y_train, cv=kfold, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M7LnhJTolVj",
        "colab_type": "text"
      },
      "source": [
        "Putting these new lines together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFUwKirKomMr",
        "colab_type": "code",
        "outputId": "9d0a53f6-df25-4439-f152-077bbf5f29de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# make the model and wrap it up for scikit-learn\n",
        "kc_model = KerasClassifier(build_fn=make_model, \n",
        "                           number_of_layers=2, neurons_per_layer=32, optimizer='adam',  # parameters for the model-making function \n",
        "                           epochs=100, batch_size=256, verbose=0  # parameters for scikit-learn\n",
        "                           )\n",
        "\n",
        "# create pipeline\n",
        "pipeline = make_pipeline(MinMaxScaler(), kc_model)\n",
        "\n",
        "# create cross-validator\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
        "\n",
        "# execute cross validation using pipeline and k-fold cross-validator\n",
        "results2 = cross_val_score(pipeline, X_train, original_y_train, cv=kfold, verbose=0)\n",
        "print(f'results = {str(results2)} \\n results.mean = {str(results2.mean())}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results = [0.96120548 0.96002001 0.96400601 0.96216667 0.96216667 0.96682781\n",
            " 0.96266043 0.96449411 0.96631652 0.95980656] \n",
            " results.mean = 0.9629670262336731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuM3HIGpyXy",
        "colab_type": "text"
      },
      "source": [
        "Cross-validation is a great way to get a handle on the quality of our\n",
        "model. It’s not so great when training times start to push our patience,\n",
        "since every fold is essentially a brand-new full-length training and\n",
        "testing process. Using 10 folds requires training and then testing our\n",
        "model 10 times in a row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU_h7x4McBqf",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Searching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leISLjZBcCpd",
        "colab_type": "text"
      },
      "source": [
        "We can use the grid searching algorithms offered by scikit-learn to\n",
        "help us out. With those routines, we can automatically try out all the\n",
        "different combinations of multiple settings for multiple parameters.\n",
        "We could do this ourselves with some nested loops, but it’s easier to\n",
        "relax and let scikit-learn do the driving.\n",
        "\n",
        "The grid searching object GridSearchCV will try out every combination\n",
        "of the parameters we give it, and measure each model’s performance\n",
        "using cross-validation. By default, it uses 3 folds to save time, but we\n",
        "can increase that with an optional argument.\n",
        "\n",
        "We think of this as “searching” because we imagine that each combination\n",
        "of parameters is a point in some very high-dimensional space,\n",
        "called the search space. Each point in search space represents some\n",
        "combination of parameters, and the value of that combination (that\n",
        "is, the accuracy or loss that results from training a model with those\n",
        "parameters) is the value associated with that point. The intuition is that we’re searching through this space, wandering from point to\n",
        "point and region to region, looking for the point that has the highest\n",
        "performance.\n",
        "\n",
        "when we prepare a pipeline for grid searching,\n",
        "we need to tell GridSearchCV where each parameter it’s searching\n",
        "through ought to be routed. This means we need to identify different\n",
        "steps in the pipeline. That’s easy if we use the pipeline-building\n",
        "method, where we give a name to each step.\n",
        "\n",
        "Let’s build a dictionary to search through three of our model’s parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a61vyt6rdGAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = dict(model_step__number_of_layers=[2, 3, 4],\n",
        "                  model_step__neurons_per_layer=[20, 30, 40],\n",
        "                  model_step__optimizer=['adam', 'adadelta'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad_73UkgfoGx",
        "colab_type": "text"
      },
      "source": [
        "We can use our dictionary with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03XT3h7OfpK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('normalize_step', MinMaxScaler()))\n",
        "estimators.append(('model_step', kc_model))\n",
        "pipeline = Pipeline(estimators)\n",
        "\n",
        "grid_searcher = GridSearchCV(estimator=pipeline, param_grid=param_grid, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzJhVS_ZgaWy",
        "colab_type": "text"
      },
      "source": [
        "Now we’re ready to roll. We just call the searcher’s fit() routine with\n",
        "our data, and let it run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQYk0pvCfjT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07f8b9a9-5265-411d-9c65-611498e0053b"
      },
      "source": [
        "search_results = grid_searcher.fit(X_train, original_y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam, total=  50.7s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   50.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam, total=  50.2s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adam, total=  49.5s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  49.7s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  48.9s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  48.4s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam, total=  53.0s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam, total=  53.0s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adam, total=  54.1s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta, total=  54.2s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta, total=  52.6s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=3, model_step__optimizer=adadelta, total=  54.5s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam, total=  57.8s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam, total=  56.6s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adam, total=  57.1s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta, total=  56.0s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta, total=  55.6s\n",
            "[CV] model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=20, model_step__number_of_layers=4, model_step__optimizer=adadelta, total=  56.2s\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  59.4s\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  59.0s\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  56.4s\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.0min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.0min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.0min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=30, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.0min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam, total=  58.2s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam, total=  58.0s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adam, total=  57.9s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  56.1s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  57.1s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=2, model_step__optimizer=adadelta, total=  55.0s\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adam, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=3, model_step__optimizer=adadelta, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.3min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.3min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adam, total= 1.3min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta \n",
            "[CV]  model_step__neurons_per_layer=40, model_step__number_of_layers=4, model_step__optimizer=adadelta, total= 1.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed: 55.1min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giMA_IMTnoe3",
        "colab_type": "text"
      },
      "source": [
        "The variable search_results we get back contains a lot of information.\n",
        "One of the objects in search_results1 is a dictionary called\n",
        "cv_results_ (recall that all of scikit-learn’s internal variables are\n",
        "suffixed with an underscore). The cv_results_ dictionary contains\n",
        "detailed information on the cross-validation results.\n",
        "\n",
        "Since we’re interested in finding the best combination of parameters,\n",
        "two dictionary items are of particular interest. The ′params′\n",
        "item tells us which set of parameters corresponds to each score. The\n",
        "′mean_test_score′ item tells us the average value that came out of\n",
        "the cross-validation for each set of parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7abL3X0gpAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "78328a2e-8c76-46c9-ddf9-c939a0d06e19"
      },
      "source": [
        "search_results.cv_results_['params']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 20,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 30,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 2,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 3,\n",
              "  'model_step__optimizer': 'adadelta'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adam'},\n",
              " {'model_step__neurons_per_layer': 40,\n",
              "  'model_step__number_of_layers': 4,\n",
              "  'model_step__optimizer': 'adadelta'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3fx2rS5qA0g",
        "colab_type": "text"
      },
      "source": [
        "The numerical data that describes our cross-validation test performance\n",
        "is in mean_test_score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj2PuLIUqBtU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e644e9e5-eb94-4eeb-ed35-d659bb0a093c"
      },
      "source": [
        "search_results.cv_results_['mean_test_score']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.94604999, 0.41956667, 0.93736668, 0.35583333, 0.92976667,\n",
              "       0.29188334, 0.95735   , 0.57238332, 0.95508333, 0.47536666,\n",
              "       0.95338333, 0.33581667, 0.96316667, 0.6476    , 0.96323333,\n",
              "       0.50271666, 0.96065001, 0.44295001])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX-mfuLpqHy9",
        "colab_type": "text"
      },
      "source": [
        "Using NumPy’s utility argmax() we can find the index of the largest\n",
        "value in this list, and then extract the corresponding element from the\n",
        "′params′ item, so we can see which set of parameters gave us the best\n",
        "score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijgtUb_D3cCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ef7079ce-197a-4a12-cf40-a6e045b25add"
      },
      "source": [
        "best_index = np.argmax(search_results.cv_results_['mean_test_score'])\n",
        "print(f'best set of parameters:\\n index {str(best_index)}\\n {str(search_results.cv_results_[\"params\"][best_index])}\\n′.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best set of parameters:\n",
            " index 14\n",
            " {'model_step__neurons_per_layer': 40, 'model_step__number_of_layers': 3, 'model_step__optimizer': 'adam'}\n",
            "′.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AjL7mq94Lh3",
        "colab_type": "text"
      },
      "source": [
        "So our best combination used 2 layers, with 40 neurons per layer, and\n",
        "the Adam optimizer. But how much better was this than the other combinations?\n",
        "Let’s plot all the values of mean_test_scores so we can see\n",
        "how every combination performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XPDHShy36m_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "db4eda64-1b6c-43ac-f585-2be3d0226c50"
      },
      "source": [
        "params = search_results.cv_results_['params']\n",
        "dict_vals = [params[i].values() for i in range(len(params))]\n",
        "name_list =[[str(v) for v in dv] for dv in dict_vals]\n",
        "xlabels = ['-'.join(name_list[i]) for i in range(len(name_list))]\n",
        "\n",
        "plt.plot(search_results.cv_results_['mean_test_score'], 'r')\n",
        "plt.xticks(np.arange(len(xlabels)), xlabels, rotation='vertical')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAE1CAYAAAD3ZxuaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO29e3xc13Xf+1sASPAN8QESDwIEIFMi\nab1AM7IT9RPJjzZSlEiO28RSG+fROGrvrWLHSW9jJ76Sr5I4adPa1w81qWL7JnVdy45jx7IrW3Yk\n2a6c2BIF6EWBlGiAhAARIEjxTfC97h/rbM/h4TzOY79mZn8/n/kM5szMmYWzZ35nnbXXWpuYGYFA\nIBCof1pcGxAIBAIBPQRBDwQCgQYhCHogEAg0CEHQA4FAoEEIgh4IBAINQhD0QCAQaBDaXH3wmjVr\neGBgwNXHBwKBQF3y9NNPH2DmznLPORP0gYEBbN++3dXHBwKBQF1CRHsrPRdCLoFAINAgBEEPBAKB\nBiEIeiAQCDQIQdADgUCgQQiCHggEAg1CEPRAIBBoEIKgBwKBQIMQBD0QCAjf/Cbwp3/q2ooSH/84\n8LnPubairgiCHggEhE9/GrjnHuDMGdeWAMzAH/4h8Od/7toS4cIFYNs24MEHXVtSlSDoAXOcPQts\n2gT8zd+4tkRgBr73PbkPXMq+fTJmL77o2hJgeho4cAAYH3dtibBvH/D008B3vuPakqoEQQ+YY2YG\n2LUL+Pu/d22J8P3vAzfeCDz6qGtLhCefBH7+5/3wiAEZLwAYGXFrBwCMjsr9vn3A/LxbW4DSiWVy\n0q0dNQiCHjDH7Kzc79zp1g7Fs8/K/a5dbu1QfP3rcttbsTWHPZhFPIGSmLokflLZs8eZGT8mCLoh\n/vZvgZtvlphWwG+UoI+NubVDoU4sPggEULJDCalLjh0DTp6Uv30Q9NFRoLVV/vYh7KJs2LvX65Bd\n/Qn67CzwyCMlsQj4i7qEn5sDDh50awtQOrFMTLi1Q6Hs8EHQlQ2rVwPPPAOcP+/WntFR4Kab5G+f\nBP34ceDIEbe2VKH+BL2/X+59ufQ5dw44fdq1FX4SP+n6EOZQgu6bh/7qq07NAFAS9JtvBk6cAHbv\ndmfLwYPy+/6ZnwGWLvVL0AF/tKcM9SfofX1y78tB/d3fBd76VtdW+Elc0F2HXY4eFeFsafFD0E+f\nlkwOwA8PXV1N3Xqr3LsMu6jPHh4Ghob8EfSrrpK/fdGeMtSfoCsP/ZVX3NqhGBkpTbYFLmZmBrj8\ncqC93f3EqPr8N75RPMBjx9za88orpVisD4KubHjLW4AFC4Kgxzl5Ur7LKgQUBF0jl10ml2G+HNTJ\nSYmrHT3q2hLhPe8BPvlJ11YIs7NATw9wxRXuPXT1+bfcIveuvXQVP29r80fQ29uBtWuBq692m7o4\nMiKO2+rVwOCgHCuXE5FqrN70JmDhQn+0pwz1J+hEMtg+eOjnzpUum32IgwLA5z8PfO1rrq0QZmaA\nri5g82Y/PPS2NuBtb5PHridG1Qnluuv8EfSuLvl9DQ+Ll+xKREdHxQZAPPQTJ2Ri3RXqCuF1r5OQ\nbxB0zfhyUPftK2UDKGF3yenTUl3ng0AA4qGvWyfVohMTwKlT7mwZGwM2bpQb4IeH3tYGvOENfjgD\n+/YB3d3y9/CwhKWmpuzbcfw48NJLwNat8nhoSO5dhl3UZw8NiTPpg/ZUoD4F3RcPPT6wPgi6EgY1\nweWS06eBw4dF0DdvlrqBl192Z8/YmJxYVq+WkJ1rD31iQr7HfX1ynFxXQ8YFXYmpi7DLs8/KlUHc\nQwfcC/qyZcCaNUHQjdDfL96fS48P8E/QlQ1zc9KTwyUqw6WrS4QUcBd2OXMG+NGP5MRCBAwMuPfQ\n9+yR+LASUdcn4ZmZki3XXCPHycXEaHxCFJCxAtwL+tBQKdw7PS3hVg+pT0FXqYsuLgnjKEFftMiP\ny+b4SWX/fnd2ACVBX7dOJkWJ3E2M7t4tobHNm+WxmmhzycSEiJUSUZdhstOngddeK9mydClw5ZXu\nBL2zE+jtlceLF4tdLsdLCToggn7hgh+/9zKkEnQiupmIdhHRbiJ6f5nnNxDRo0T0HBF9h4jW6zc1\nhi+pi5OTwMqVMtg+eeiAHx4fIIK+ZAmwYYM7D12dSNSVgmsPfX5eTnhxD92loKux6uoqbdu61U3I\nZWREvHOi0jaXqYvMlwo64Ef/nTLUFHQiagVwP4BbAGwBcCcRbUm87D8D+O/MfA2A+wD8iW5DL8KX\n4qLJSRng3l4/BD1+xeJ6YjQecgFETF156OpEogR9cFDKtw8dcmOPOpnEPXSXHp/6rihbABHVqSmZ\nZLfFmTPAjh2lcIvCpaDPzEhoNynorrWnAmk89OsB7GbmcWY+A+BBALcnXrMFwGPR34+XeV4v66ML\nAB88dJ8EfXpavGHAvYeuBH3tWrnfvFnK/100VRsbEydg2TJ5rOKyrrx09bmDgxJeaG11ewKuJOiA\n3bDLjh0y96MmZRVDQ/Jbd9FmOJ7hAvjjTFYgjaD3Aogr51S0Lc6zAN4R/f0LAJYT0eri5lVg8WL5\nIbg+qHv3lgR9ZsZ9Q6PpaclrBtx76DMzUgS2aJE83rRJQg0uxmxsrBQ/B0RIAXeCruLBg4PSiqCr\ny4+QSzlBtxl2UZ9VzkNndhPmSAr60qWSKeVaeyqga1L03wO4kYhGAdwIYBrAJepGRHcR0XYi2j5X\ntFDAderikSNy6++Xasjz591PRE5Pyxdv5Uo/PPR160qPlaDajqNfuCCfqcItQMlDdzXRNjEhVZnq\n+HR3u/fQW1pKV1MAsGqVzHvY9NBHR4Hly6VdRBx1AnYxXuPjEs/fsKG0zePUxTSCPg2gL/Z4fbTt\nxzDzq8z8DmYeBvAH0bbDyR0x8wPMvI2Zt3V2dhYwG+4PqjqZbNhQmpF3GXZhljhsb694fK4FfWbm\nYkFXgmo7jj41Jb044h76ypVAR4fbkMvAgIgo4Iegq9BPHFUxaovRUbnCbEnIkstc9PFx+U2pK01A\nfvN1LOhPAdhIRINEtBDAHQAeir+AiNYQkdrXBwB8Rq+ZZVDVoq7Kk9WAqpAL4FbQDxyQGGNvr3uB\nAMRDj2dNdHbKpaptD12dQOKCDoiguvTQ1VUCIFd4ridF4+EWxdatUrVpo5HZ+fPShz0ZbgHEtvZ2\nd4KuTigK185kFWoKOjOfA3A3gEcAjAH4IjPvIKL7iOi26GU3AdhFRC8BWAfgjw3ZW6K/322zed8E\nXX22Lx56MuQCiJduW9CTGS6KwUG3HroKIwAiWOqE7IJKgq7E1UY30ZdfliupcoLe0iLHyydBP3rU\ny4UuUsXQmflhZr6CmS9n5j+Ott3DzA9Ff3+JmTdGr3k3M5tf8cH1bPPkpPTi6OqS2GNrq1svKy7o\nykN3dfVy6pR82eMeOuAmdXFsTEIs8fgwUPLQbR+jY8ekT0rcQ1di6moVrlqCbiPsoj4jmeGicJG6\nOD8vv+lygg546aXXZ6Uo4L64aHJS0idbW+XW1eXWQ1c56MpDP3XKXUvfeJVonM2b7S9HpzJc4oUq\ngAjqiRP2l8aLpywqXBYXqcn8coLe0yMnQhuZLqOjElZJhsYULqp71VgFQbeADx66GljAfS769HQp\nBU55xq7CLvEq0TguerokM1wUrjIn4imLCpeCfuCAiHryagq4uJWuaUZGZEWgBQvKPz80JE3MbBaD\nJVMWFUHQDdDVJSEPlx66b4K+bp0cE9fl5MkqUYXt1MXXXhPvs5zX56q4KF4lqujpkXsXIbtyRUVx\nhoel4MfkurnMctKoFG4B3GS6VBL0devkxBMEXSOtrRLycHFQz5+XEEdc0F1nKkxPlypoXXvolUIu\nGzbIZbWtOHqlDBfAXS76xIQUp6xZU9q2dq14wy5OwLUEfetW6Sz4wgvmbHjlFTn5lpsQVbgS9CVL\nLp1/aWnxZ02GBPUr6IAcVBceulrYIumhHz4sM/UumJ4uZdu49tDViST5Q2htlS5+tjz0ShkuALBi\nhRTP2PbQVcpiPKbf1ibHykdBtzExWqlCNI4KUdkWdNU2N4mnqYv1LeiuDmo8ZVHhOnUxLugrV8ol\noUsPfeVK8caT2ExdHBsTG+LhjTguUheTKYsKV7UD5TotxhkakupNk4I+Oipe7zXXVH7N8uVyVWPz\niqpcyqKiv9/Ljov1Leh9fRL6sN1DxTdBn5+XySJlA5HbXPRyOeiKzZvtLUc3NiZXBMkKSIXt4iLm\nS4uKFK4Efd8+qZpdvLj88y0t4jmbzHQZHZUTvWosVwmbqYvJtrlJPF3oor4Fvb9fDqjt/F0l6H2x\njghKTF3E0eM56AqX1aJqcehybNpkbzm6ShkuCuWh28pFP3xYUknLeeiu5mAq5aDHGR4GnnvOnOOk\neqDXwqag798v4dNqgu7hQhf1LeiuUhcnJ6WT4IoVpW0qU8GFhx7PQVf46qHb6ukyPy/ecKW8ZkA8\n5VOn7DkE6mqgkoe+f7/9q820gn7ypLQB0M3cnPxmqmW4KIaGJMxhwyuulOGi8DR1sb4F3VVxUTJl\nERBxX7bMjaDXk4eulqMzHUd/6SXxvGsJOmAvjl6uqEjR3S0en+2OnWkE3eSi0ck1RKsxNCRibmPp\nySDoDnB1UFUf9CSuctHLCXpXlxSN2F4sen5eytsreehqOTrTHnq1DBeF7eKickVFCheZScwXLw5d\niU2bZHLZxMSoOkmoPv7VsDleStArTagHQTdAR4d4xS489Hh/ZEVvr7sY+ooVkgmgUD9S2x5fpRz0\nOJs3m/fQx8bkSuCKKyq/Ro2hTQ+9o0PCdUlcCPrRo3ICriXoCxYAV19tRtBHR0WoV66s/Vqbuejj\n4xJGrTRZ7OlCF/Ut6ET2UxePHpXJrXIeek+POw+9N7GIlAp52A671EqDA8TjM70c3diYCEWlHyQg\nP8q1a+166OW8c8BNtaj6blQbK4VaNFr3BPLoaLpwCyCFc21t9gS9UrhF4WEuen0LOmC/YktdDVQK\nubz6qv11M6sJuu2J0bQeuunl6GpluCgGBux56JVSFgE3J+BaRUVxhofFkdGZe330qGQ7pRX0tja5\nqgqCXpH6F3TbS9GVy0FX9PZKzNrmSulAeUF3VS1aqTFXHNNNus6flyuAahOiCltd/JgrFxUBwMKF\ncgnvs6ADesMuqs96mgwXhY3UxVOnSss5ViMIugH6+iRObKNQBagt6IDdy+bz5+WHmRR0JaiuPPRk\n2X8c06mLe/ZIM6k0gj4wIF6n6auquTlJ/avkoQP2M5OyCPo110iBls5MlywZLgobJ+C9e+UEnEbQ\n1drCnlD/gq6E1UYqEyCC3tpauX80YDeOvn+/pHIlBb29XXqVuBD0VavE46yE6eXoqjXlSjI4KFdV\npoW0Wsqiwragz8zIWpkdHbVfu3ixnIh1eugjI+J4pDmhKIaG5ORoclm8WimLCtdrMpShcQTd1qVP\nfGGLJC7K/8ulLCq6utyEXNJMsplcvShNyqLCVtfFakVFCtvVovv2yViVaz5VDt290bNMiCqUyJoc\nr6yC7lHYpf4FXVWL2jpLlisqUqgfh0+C7sJDrxY/V5hMXRwbk5DPqlW1X2uruCithz4zY29SPU1R\nUZytW+WEo6Oy9tQp4MUXs8XPATupi+PjcuVSyzEJgm4A1QPcpodeSdAXLBAxcyHo6jjEcVEtOjOT\nTtA3bTK3HJ1adi4NKhfdhoe+Zo3UTVSiu1vCZ7aWxcsq6DonRl94Qf7XvB66aUGv1DY3TleXdwtd\npBJ0IrqZiHYR0W4ien+Z5/uJ6HEiGiWi54joZ/WbWoFFi8Qbs3FQyy1skcT2ZfP0dKmfdhLlodtc\nCHl2Nl3IxdTqRczpUxYB+f50d5v30KulLCpsZyZlFXRVzalD0PNMiAJSgNTRYT7kUivcAkgnyvXr\nvWqjW1PQiagVwP0AbgGwBcCdRLQl8bIPAvgiMw8DuAPAf9VtaFVspS7OzIhXUU3QbZf/T0/Lj7Kl\nzFB2d9tdLPrECeD48fQeOqBf0Pfvl1bCaT10wE7mRLWURYWaVLch6KdOSV55FkG/7DIROh2ZLqOj\nIsxphDOJydTFWm1zk3iWupjGQ78ewG5mHmfmMwAeBHB74jUMQLUe7ABgt/7dVnFRtZRFhQtBLxc/\nB+wXq1RaS7QcGzaId6x7YjRLhovCdHHRhQuy/7Qeuo0rvDQVveXQNTE6MiIef9oJ2TgmBf3AAXFK\nGljQewHE3d+paFucDwH4ZSKaAvAwgN/SYl1alIduOrSQVtAPHrSXF59G0G1NjKapElW0tkqfFd0e\nepYMF8XgoIytqbasMzPAmTO1PXSbIZcsOehxhoeBH/2oWO71+fPSXz1ruEUxNCRXVCYmj9NmuCg8\nW+hC16TonQD+ipnXA/hZAJ8lokv2TUR3EdF2Ito+Nzen6aMhB/X4cbmENElaQQfsecVTU5UF3XZM\nNougA2ZSF8fGpEdLfPGRWgwMiMiYurJKk7IISK53R4ffgq6yUp55Jv9n79olrR+yZrgohoakcMzE\nccoj6Kq4zwPSCPo0gPivY320Lc5vAPgiADDzPwJYBGBN4jVg5geYeRszb+vs7MxncTlspS5OTsoP\nLr6wRRKbxUXHjsnNFw8962W8ieXoxsbkRJHlUt50W9Y0KYsKW5lJRTx0oFjYJe+EqMLkgtFqn2nG\nCihlSXkSdkkj6E8B2EhEg0S0EDLp+VDiNZMA3goARLQZIugaXfAa2MoH3bu3fNvcODaLi6rloAOS\nEbBwof2QS9qT9aZNEibTuRJOlgwXhelcdHWiqPXdAexlSc3MyER6Vseqq0tuRQR9ZETmT7KOk8Jk\ncdH4uPx/tdY3VXiWi15T0Jn5HIC7ATwCYAySzbKDiO4jotuil/0ugN8komcBfB7ArzFbzJWz6aFX\nC7cAbgS9XA46UFos2tbl4MyM5FovWJDu9bpTF48fl+9AlglRQL4/LS3mBH3PHhmHaq18FTY99HXr\nKi+gXY2ii0aPjkpvmLa2fO/fsEG+26Y89CyZN66WwaxAqiPKzA9DJjvj2+6J/f0igBv0mpYBWwn+\nk5PADTX+zcsukx+uDS+rlocO2K0WTVslqtC9HJ3aT1ZBX7hQjqGpkEu1PuhJlKAz58sASYsq+8/D\n1q3At74lcfA0J6k4zCLo73xnvs8GZLz6+swJ+k//dPrXL1smFcmeCHr9V4oC4l319po9qMeOSX5z\nLQ+dyN5CF2kE3Wa1aFZBX7xYwh26JkbzZLgoTKYupikqUnR3y4Sf6Qn+rEVFcYaHZSLwhReyv3fP\nHvnf8sbPFSZSF8+ckSu8rLnxHqUuNoagA+aLi6otbJHEVi769LTEyat5STY99LSNueJs2qTPQx8b\nkxDC616X/b2miovOnZPvThYPHTB/Ei4q6EC+sIuKvefNcFEMDuoX9LRtc5MEQTeA6YOaJmVRYVPQ\nq3nngPxobS0WndVDB/QuRzc2JmJerXVvJQYG5HieOVPcjjgqRzmth25jKbrz56WiNq+gDw5Ktlee\nidHRUTnpXn11vs9WDA3JSWl+vth+4mRNWVQEQTdAX5/8eM6fN7P/rIL+6qvmC52q5aArlMeso0Ne\nNY4fl9L/rB66zuXo8mS4KAYH5aSi+yovS8oiYMdDn5uT/zWvoBPlrxgdGZExX7Qo32crlOjqDJMV\nEfTDh+212KhC4wh6f794QqbCC9UWtkjS0yO51YcOmbFFkcZDt5WLnrWoSKFr9aKzZ2V9yqwTogpT\nqYtpi4oUNgQ9y+LQlRgelmrPrBWSo6PFwy2Ama6L4+OyMEzWE51HC100jqCbTl2cnBTxTJNqZSN1\n8dw5EdE0IRfAfEw2r6DrSl380Y/kmOQVdFPFRXv2iEeb5soOAJYvl0pXG4Ke10MHRJRPnco2bjMz\n8tlFJ0QBc4I+OFi+0V011Nh60HWxcQTddIJ/mhx0hQ1BVwsh+OKh5232tGaNnuXoimS4AHIcW1vN\neOjr12eL65vOTNIh6HkqRotWiMbp7JTiH92Cnqf7o0fFRY0j6KYT/H0T9FpFRQpbi0Xn9dAB8aqL\nhlzU+/MKelubjK9uDz1LyqLCdLWojpDLlVdKHDxLposSdNVXvQhEpSZdOsjaNjdOV5d8f4Kga6Sj\nQy5XTYRc0ixsEcdGG9Q0OehAabFo0yGXmRn5keXp0aMjdXFsTI5FtT47tTCRi56mD3oS0x76zIyk\nuxaZmGxrA669NruHfvnl6RalToPOXPTXXpNJzTyC3toqjlUQdI2oOKWJgzo7K5NuaQW9vV1CCTY8\n9FqCDpTWqjTJ7Kz8z3nKuTdvLr4cXZEMF8XAgF4P/cwZcQSyeug2Qi5FvHOFynRJm3I6MqIn3KJQ\ngq4jmyxvhovCk9TFxhF0QMIuJjz0LCmLCtO56FNTEpddc0lTy0ux0c8lTw66oujqRWrZubwToorB\nQTlOuro/qh79eTz0EyekOtkERYqK4gwPi1eb5iR45IiIpo4MF8XQkBwnHa241f+QV9A3bAiCrh1T\nZ0kfBX16WmKtafp92KgWzVMlqiiaujg9LeJXVNCVJ63rO5Q1ZVFhOjNJl6ArcU4TdlH903V66Drb\n6GZtm5ukv1+cLFN1MClpLEHv65Oztc7qMSC/oJuOoacJtwClkIvJQqciHrpaji6vh563KVcS3amL\nWYuKFCarRZn1CfpVV0n8OI2gq8lT3SEXQM94jY/LQuvLluV7vycLXTSWoCvBnZrSu9+9e2UiJ8tk\nTk+PlFebKrnPIuhdXRJGKLJsWDWY5YSRV9CLLkdXNMNFobu4aGJC/re046Qw6aEfPizNv3QI+qJF\nwJYt6TJdRkflN5H3O1IONV66PPS84RbAm9TFxhJ0U6mLWVIWFb29JW9IN8zZPXTAXNjl+HG5Kioy\n0VYkdXFsTE62RSf6enqkDbNOD72/P/tEsUlBV98BHYIOSNgljYc+OqrXOwckD727Owh6jMYSdFMl\nuHkFHTATRz9yBDh5snYOukIJnanLwSI56IpNm/IvR6cmRIv2D29pkfCPTg89T0z2ssskU8rEeOnI\nQY8zPCzjX83W+Xk56eoWdEBP6uLZs/IbLyLonix00ViCrgTOFw8dMBMHzZKyCJivFs1bJRpn8+b8\ny9GpdUR1oLONbp6iIqDUU9+koOvy0NO00n3+eYkv68xwUehoo/vKK2JfEUFfvlxy+4Oga6S9XbxE\nnR768eNSdJBV0E0uFp1V0E2HXHR56ED2OPrhw/J/FZ0QVegqLpqfF7vyZk10d5txBnQLuqr6rBZ2\n0Vnyn2RoSObMirQ9LpqDrvAgF72xBB3Qf1CzLGwRZ80ayRM3Iehq0jetoJu8hAf0CLpaji5rHF1X\nhotiYEAms0+cKLYf1agpj4cOmCsu2rdPFkQpUlEbZ8UK6UFfS9BXrky3SHZWhoaksKnIbz4Iusfo\nLi7Kk7IImF2KTu1TXQWkscVkLnreFeTjqOXosnroujJcFMqjLto5L2/KosKUoM/MyL51rldaa9Fo\nVSFqYo1UHV0Xx8fF+Ur7e6pEf7/zjouNJ+jqLKkr5zqvoAPmctGnp+UKoL09/XtMVouqsv88K8jH\n2bQpu4c+NiY/xrzCmUR51EXj6HmLihTd3TL5rbumQlfZf5ytW+UEVq7//7lzEkM3EW4B9An6wEDx\n768HC12kEnQiupmIdhHRbiJ6f5nnP0pEz0S3l4jI8Aq3Vejrk8tlXYtLTE6K95nn7G3SQ8+a22za\nQ9chEps3Z1+ObudOCdfk6SFTDnViKBpH37Mn32IJCvV9030S1lVUFEeJtaoGjbNzp2QumRL07m45\nzkUFvWi4BfBioYuagk5ErQDuB3ALgC0A7iSiLfHXMPP7mPk6Zr4OwCcAfNmEsanQfVCzLGyRRJX/\n667QzCPoJht0FakSjbNpk/z4s8QhdWa4APJ/LFpUXNAnJiRmnHWxBIWpjp0mBb1c2EVtM5HhAsjx\nLdpUTbegO4yjp/m2XQ9gNzOPM/MZAA8CuL3K6+8E8HkdxuVC90HNk7Ko6O2VqwXdl2DT0+lz0BVd\nXbJYtO5FkAF9gq4mNtOGXU6dkh+jrglRQOK8Orou7tmTP9wCmCkump+XMI5uQV+7Vr7r5SZGR0el\nAOiKK/R+ZpwiueiHDsmtiQS9F0Dc3Z2Ktl0CEW0AMAjgseKm5UT3UnRFBR3QG3Y5c0ayMPKWk+/f\nr88WoFT2ryPkkjV1cfduCc/oFHRAT+pi3qIihQlB110lGqfSotGjo8A11xSPT1ejiKAX7bIYp7vb\n+UIXuidF7wDwJWYu23KMiO4iou1EtH1OR8vLcqxbJ+XbOg6qWgW+qKDrvGxWP/A8MfT4+3Vx7Jh4\nyjo89DVr5JbWQ9ed4aIoWlx0/LhcDRXx0FevFnHQOV66q0TjDA/LifjkydK2CxfMlPwnGRqSycg8\n82a6UhYBLxa6SCPo0wD6Yo/XR9vKcQeqhFuY+QFm3sbM2zqLpLhVo6VFDqoODz3rwhZJTBQXZc1B\nV5iqFtVRJRony+pFY2MSIrnySj2frRgYKK1gk4eiKYuAfI91py7qLiqKs3WrCPhzz5W2TUzIMTQV\nP1cUaaNbtG1uEse56GkE/SkAG4lokIgWQkT7oeSLiGgTgJUA/lGviTnQdVCLpCwCZkIuWatEFaaq\nRXUUFcXJkro4NiYTj0uW6PlsRdGui0VTFhW6q0VNCnq5RaNNVojGKdJGd3xcrgp1FVr5LujMfA7A\n3QAeATAG4IvMvIOI7iOi22IvvQPAg8wmm26nRFdxkRqYvBVuixdLhZwPgq4EV3fIRbegb94s4YoD\nB2q/Vseyc+Uomrqow0MHzHjora3FCsAq0d8v3/V4psvIiISNrrpK/+fFKeqh6wi3KBwvdJEqF4+Z\nHwbwcGLbPYnHH9JnVkHiB7XIZIyq+srroQP6i4umpyWtbuXKbO9buFDisvUQcgEkH73a8noXLshr\n3vxmPZ8bp2hx0cSEXDUUFc7ubuCJJ4rtI86+fXLizZtKWQ2iS1vpjo4Cr399tgK4PKxYId+VvIL+\nEz+hz5b+fimmmpnJ7nRpoPEqRQHx0HWsHjI5KV+WIquU6y4uUjnoecqoTVSLzs6KQKxerWd/aVMX\n9+6VNDzdGS6AiMPSpcU89E4m6LgAACAASURBVIGB4qXu3d2ycLauVFNV9m+K4WGpCj17VrKfdC8K\nXY08mS7nzsn3SLeHDjgLuzSmoOsqLiqSsqjQvbZonqIihYlq0ZkZ8UR1paX196dbjk49byLkUjQX\nvWjKokJNqusaMxNl/3GGh+Xk8+KL8ln799sT9DxtdKemRNSDoHuOroOqS9BnZuSLo4M8RUUKE9Wi\ns7N6RaK1VbJWanno6nkTHjogAlFkUrTohCigv1rURJVonPii0Sr0YjrDRTE0JN52lti1zpRFheOF\nLhpT0HUVF+kS9AsX9BT0ZF16LokKueict9ZVJRonTeri2Fgpb90EeYuLDh+WakwdHrrO4qJz5+Q7\naFLQN26UuQMl6ETAtdea+7w4Q0PyP2ZZT9iEoK9YIe2qHXVdbExB7+iQA1vkLHnihMQviwq6zlz0\ngwdlgd+8gt7dLe/XuVi0rirROGo5umqdBk1luCgGB+U4ZS1W0ZWyCOgV9P375URuUtBbW0XAR0bk\n9rrXyUo+NsjTdXF8XLJw8l7xVsJh6mJjCjpQPHUx78IWSXTmoudNWVTorhZlNuOhq+XoXn658mvG\nxsyFW4D8uei6UhYB6ZHS0qJnvEyW/cfZulW6Lo6M2Au3APkFXUfb3CRB0A1Q9KAWLSpS+CTououL\njh4Vj99EyAWoHHaZm5OrFZOCrgQ568SoTg+9tVWOrQ5BN1n2H2d4WFof7N1rb0IUEC+7tTW7oOsM\ntyiCoBugr88PQV+7Vr5oOia2fPPQdeegK2otR6d72blyFPHQV6zIXidQCV3VoiarROPERdymoLe1\nSQGgL4J+6JD0ObJM4wp6f79UG+Zd8aXIwhZxVE8OXR46Uf4fpe5+LrqrRBW1lqMz1ZQrzmWXiTDn\n8dB15KArdFWL2vLQX/96aY4H2BV0QMQ57XgdOSJXeaYEHXCy0EVjCzqQ/6BOToqYqy9nEXTlok9P\ni8ef1ya1WLQuQTfloQPifVfy0MfGJJui6NVTNYjypS7u2aOv0ROgV9BXrTJftdneLqK+fr2ZFgPV\nyFJcpLNtbhLVKsRB2KVxBb1o6qKOlEWFTkEvMiOvFovWFXIx5aED4n1XWo5u507JVTdRwh4na3ER\ns76iIkVPj2SoFK1jMJ2DHufee4EPf9jOZ8UZGpL5lTShDhMpiwqHxUWNK+hFD6puQdcVQy/aH0Jn\ncdHsrMwP6Cr7j7N5s/RZL5fPq3vZuUooDz1t3v6BA5LuqmNCVNHdLZ9ftI7BRHppJd7+duBd77Lz\nWXGyTGSbFPTubvldBEHXiBK+PB560YUtytly5Ij82IswNVVc0HV66DMzpdQ63VTKdDlxQkTe5ISo\nYmBAPi9N50dAb8qiQle1qE0P3RVZUhfHxyUEVaRPUyUcLnTRuILe3i7ileeg7t8vPSnyts1NoqO4\naH5eFl3QIeg6PXQT4RagsqC/9JLc2xD0rG10daYsKnQUFzE3l6Cn9dBNeOcKR6mLjSvoQP7URV0p\niwoduejKQ9MRctHVwc/kZXyl5ehsZLgosqYuqtf5JuiHDsl4N7qgr1wpHndaD13nlVSSIOgG6O/P\nF3LR0Qc9jg5BL5qDrlACrCY0i2DSQwfK93QZG5MQz8aN5j5XkbUv+sSEXMbrWv0G0FM7YCsH3TVE\n6TJdzp+Xk69pD93BQheNL+iTk9mbUen20FXIpUgcVJeg66oWNVX2H6dc6uLOncDll5tPvwNEmFet\nyuah6/b6FiyQ9L8igm6r7N8H0rTRnZ6Wnu2mBf3sWT2OUwYaW9D7+mQV8qwNliYnpamQrgmT5cvl\n5pOHXnRi9PBhuYw3mTmxadOly9HZynBRZEld1J2yqChaLWqrqMgHVHFRuXRXhckMF4Wj1MXGFvS8\nB1WlLOqq9gOK56JPTwPLlhW/nNdVLWoyB12hJj5V2OXcOZkUtTEhqkhbXMQsoTqd8XNF0eKiZgm5\nACLSp09X/37bFHTLbXQbW9DzFhfpzEFX6BD0vEvPxVECXFTQTVaJKpKZLhMTchlrU9BVX/RaYbuZ\nGcmbN+WhFxX0JUvstbJ1SZrUxfFxSS1U+mACnz10IrqZiHYR0W4ien+F1/wSEb1IRDuI6H/qNTMn\nRT10nRQtLtKRgw6UFosuGnKx4aFv2CDL0ak4us0MF8XgoAh1rVioiZRFRU+PfH61MEI1VMqizitO\nX0kr6P39etp6VEKtReyboBNRK4D7AdwCYAuAO4loS+I1GwF8AMANzPx6AL9twNbsqL4nWTz0kycl\nZqtb0Ht6RNDz/ih1VIkqdFSL2hD0lhYp8Vceuull58qRNtPFRFGRortbwk1pC5ySmF4c2idUqLSW\noJsMt8Rt8U3QAVwPYDczjzPzGQAPArg98ZrfBHA/Mx8CAGbWsN6aBlpasuei61rYIklvr/wo5+ay\nv/fCBTkZ6BJ0HdWiMzPSsnTVKj02VSKeurhzpwiTieq+SqQtLlKCr6sYLU7RalHTi0P7RHu7VGkG\nQa9IL4C4izsVbYtzBYAriOj7RPQDIrpZl4GFyXpQdacsKorkos/NyclAp6Dr8NBNlf3H2by5tByd\n7QwXoCTQaTz0deskVq2bosVFzVAlGqdaG91jx+T3ZEPQN2zwUtDT0AZgI4CbANwJ4C+J6LLki4jo\nLiLaTkTb5/J4qnnIuhSdaUHP42XpSllUqJBLkcWiZ2bMhlsUmzaJnS+9ZH7ZuXIsXSonrjQeuon4\nOVBM0E+elJWlmk3QK3noJtvmJunvl3Ydx4+b/6yINII+DSA+Hbw+2hZnCsBDzHyWmScAvAQR+Itg\n5geYeRszb+u01Su5v18EMW3F1uSkxOB0iaeiSD8X3YLe1SWpXYcP59/H7Kydy3gl4I8/LsJkW9CB\nUqZLNUwUFSmKCHozpSwqhobEcSq3uI2NlEWFg4Uu0gj6UwA2EtEgES0EcAeAhxKv+TuIdw4iWgMJ\nwWRYC8ogfX0i5ml/DDoXtojT1SXhiSKCrmt1ch3VoqarRBUbN8oJ9itfkce2Qy5A7eKi8+fle2NK\n0Bctkj4lQdDTUW3ew4WgWwy71BR0Zj4H4G4AjwAYA/BFZt5BRPcR0W3Ryx4BcJCIXgTwOID/i5kP\nmjI6E1kPqomURUAmENetyy/oasFgHRStFlVl/zY89MWL5Qf6xBPy2IWHPjgoBSKVMpRUKbmpkAuQ\nv1rURr2Ab1RLXRwfl0l1XWu+VsOBoLeleREzPwzg4cS2e2J/M4DfiW5+kbW4aHISeMMbzNiSNxd9\nakp+kK2teuwoWi166JAImA0PHRCvfHxcCmOKrvGah4EB+X9ffbX8VZLJlEVF3uKiZvTQq7XRVRku\nNnLyHSx00diVokC2s6Ra2MJE6hmQv1pUZw46UDzkYiMHPY4Ks2ze7KY4plbqosmiIkURQW9rk1bE\nzcLatZJtVMlDtxFuAeS49/YGQddKloqtuTmZLDS1+HBPjx+C3tEh+bp5Qy62L+NVmMVF/ByoXVy0\nZ4+caEwuWt3TI+OVNTNp3z458ZpOL/UJtcB3UtAvXJAxtCXogPVc9OYY5bSpi7r7oCfp7ZU0pnKz\n79XQLehExapFXXroLlBXbNU89J4esy19u7ulu2XWzqHNloOuKJe6+OqrcgyDoNc5aQ+qqRx0RZ5c\n9BMnZD1S3WmURapFbXvoW7cCt90mNxcsWiSiWM1DNxk/B/JXizZT2X8cJejxKxqbGS4KtchO3pYf\nGWkOQU/rofso6Lpz0BVFPfQFC+xkCgASD/3qV4EtW2q/1hTV2uiaLCpS5M1Fb6ay/zhDQ5cu8O1K\n0M+e1beObw2aQ9D7+2VgT56s/rrJSek5ftklRa56yFNcZErQi5T/q7L/Zujep6iUi372rGQh2fLQ\nswi66h3UrB46cHHYZXxc5hJMznUksZy62ByCrlIXp6aqv87EwhZx8vRzUTbrKipSdHXlXyza5OLQ\nvjIwIFd5585dvF1dTvso6LOzEnJoRkFX45EU9L4+aSFtiyDoBkh7UE0VFSk6OiR84IOHrn7kedY8\ntFUl6hODg1IRmhw7GymLgFw5Ll+eTdCbMQddUUnQbYZbgCDoRvBF0FWPmKwx9I4OaRKlkyLVos3q\noQOXhl1sFBUpslaLNrOgL1ki31HXgt7RYXWhi+YQdLV0W7WJ0fl5iTeajq9lzUXXnbKoyFtcdOEC\nsH9/c3rowKUToxMTUg2oOyRWjqzFRc1Y9h8n3kb3xAm5srQt6IDV1MXmEPSFC+VLXe2gmlrYIknW\nalFTgp63/P/QIYkjN5ug9/WJU1DOQ+/rk6pA06jiorSo1zazoCsP3Wbb3CRB0A1QK3XRdMqiQoVc\n0lb8mRL0tWvlPmvIpVm9voULxQsv56Gbjp8rlIee9ruzb5+sH2tzEtAnBgflN3/mjJuURUUQdAPU\nOqg2Bf30aakYrcX58yKgJgR94ULp75HVQ7ddJeoT5VIXbRQVKbq7JfX22LF0r2/WKlHF0JCECCcn\n3Qv6wYMS9jFM8wi68tAreTemFrZIkiUXfXZWRN2UTXmqRZvVQwcuLS46dUqutmwKOpB+YjQIutyP\nj5e6da5ebd8OiwtdNI+g9/eLd1PJM56clC+/6cvTLLnopnLQFXmqRZvdQ5+aKuXuq94/NkMuQPqT\ncDNmI8VJCrqttrlJLKYuNpegA5XPkpOT5trmxski6KZy0BV5qkVnZ+WkZ6qa1mcGB+UKT32HbKYs\nAqWruzSCzty8fVwUPT3yXZ2YcJOyqAiCbgBVLVrpoJrOQVdkuWy2IehZW7KqxaGbqexfkcxFt1VU\npMjiob/2mlxJNLOgt7TI2Ozebb9tbpyeHrElCLpGqp0lme0J+sKFkmGS1kNfsAAwtaC2asmaZbHo\nZqwSVSjhVp75nj0yPrZWUVqxQpbkSyPozVxUFGdoCPiHf5D5DleCrha6UCE6gzSPoHd2ipiWC7ns\n3292YYskaYuLpqflB2lqcYI81aLNLOjr10sRkRL0iQkJ09laPEL1sU9zdRcEXRgaKoUVXQk6YC11\nsXkEvaVFwi7lDqqtlEVF2uIiUznoijzVos080dbWJt8hFWqxmbKoSFstqsY0CHr5v23jk6AT0c1E\ntIuIdhPR+8s8/2tENEdEz0S3d+s3VQOViouaVdCzVos2a9l/nHjqos2iIkXaatFmrxJVKBEnspP0\nUAlLC13UFHQiagVwP4BbAGwBcCcRlVtp4AvMfF10+5RmO/VQ6SzpQtDn5qq3rmWWFDkbgp425HLw\noOTFN7NIqOKiEydkDH310Pftk4Zuy5ebt8ln1PisX292icBaqIUu8nQ3zUAaD/16ALuZeZyZzwB4\nEMDtRq0yRX+/xB+TPa0nJ+XLb2sFnjTpZ0ePimiYbPrU0SHLq6X10Js5B10xOCjjtnNn6bFNurtL\n341qNHtRkUKNj8twC1C6OjAcdkkj6L0A4nGKqWhbkn9ORM8R0ZeIqE+Ldbrp6xMPMymkphe2SJIm\nF910yiIg/2+WatEg6KUQy3e/e/FjW6RNXQyCLnR0iAPlaoFxhaVcdF2Tol8DMMDM1wD4NoC/Lvci\nIrqLiLYT0fa5uTlNH52BSgfVVsqiwhdBB7JVizZz2b9CeXyPP37xY1sEQc/OY48Bf/RHbm3wSNCn\nAcQ97vXRth/DzAeZ+XT08FMA3lBuR8z8ADNvY+ZtnaZyq6uhiouSE6OuBL1a+pktQc9SLRo89JJH\n/r3vSU646lppi7TVos2cjZTkyivd9HCJ09EhdQQeCPpTADYS0SARLQRwB4CH4i8gorgrcBuAMX0m\naqRctej8vGRu2BT0VatkgiaNh266aCVLyGVmRuzu6DBrk890d0sx0dGjIu62K2bTeOgnTkhHxuCh\n+4WF1MWags7M5wDcDeARiFB/kZl3ENF9RHRb9LL3ENEOInoWwHsA/JopgwuxYoWIUdxDVw2wbAo6\nUe3ioulpEf7Fi83a0t0tZeKnT9d+rSoqasayf0Vra2mCy3b8HJDvxMKF1QU9FBX5iQVBT7XMCjM/\nDODhxLZ7Yn9/AMAH9JpmiORBtZ2yqKiVi246B12hLstnZ2sfg2auEo2j+oPYjp8DpYnsauG6IOh+\n0t8PPPmk0Y9onkpRhUrwV7gU9FoxdBuCnqVaNMRlBSXkLjx0oHYuehB0P+nvBw4ckDbehmg+QU+W\n/6uFLWws8htHhVwqdTo0XVSkyFItGjx0QQm5Cw8dqF0tGrKR/MTCQhfNJ+hqOSh1lrS1sEWS3l6x\n4ciRS587e1Ymam2cZNJWi54/LzYFkQCuuELuN2508/lpPPS2NveZHYGLsZC62HyCnkxdtJ2yqKiW\ni656lNvw0JXHXctDP3hQ+lAEDx14+9uBb38buPZaN59fayJ73z458drqAhlIh9IZg210m2/Ek2dJ\n14JeLo5uKwcdkBS8NWtqe+ghB71EWxvwtre5+/xaqYuhqMhPLCx00XyCHvfQbS5skaSah25T0IF0\n1aIhLusPtYqLgqD7iVoMJQi6Rnp7ZRJ0clK65Z065UbQ1Q/OB0FPUy0aPHR/qOWhh2wkfzGci958\ngr5wofwgXnnFXcoiIAVDq1ZVFvT2dnuTWmlasgYP3R+qCfrZs+KoBA/dT4KgG0ClLroUdKByLvr0\ntFya2arIVB56tcWiZ2el1W6z99f2gc5OqVgtJ+jqSioIup9s2GB0oYvmFHRVXOSDoJfz0G3loCu6\numSxjUOHKr8mlP37Q0uLjEU5ZyAUFflNf7/81vbvN7L75hR05aHv3QssWSKhDxdU6ucyPW230ClN\ntWiIy/pFpeKiIOh+YzgXvTkFvb9fuiyOjtpd2CJJb694vvEVlJjtlf0r0lSLhipRv6g07xEE3W+C\noBtApS4++aS7cAsgon3hwsVCeuiQZN7YFPQ0LVmDoPtFJUFX3yXbfdoD6ejvl4SIo0eN7D5Vt8WG\nQ4n4/Lx7QQckFqpCLLZTFoHaHvr585I5EUIu/tDdLWNy9qzkNyv27ZNCMdutLALp6OiQfvWGogLN\n6aHHRdyloKsCkXgc3YWgr1ghGSyVPPQDB0LZv290d0t4LrmKfCgq8hsioyHe5hT0zk7J8wb88NBd\nCzpR9WrRkIPuH5WqRYOgNzXNKehEpTi6S0Hv7JTL5XKCbnrpuSTVqkVDlah/VJr3CILe1DSnoAMl\nQVfLibmgpUV+fPF84qkpEXrbMdBq1aJB0P2jnKBfuCBjFa6kmpbmFXSVrmgztFGOZC667Rx0RTUP\nPYRc/EMVecUF/bXXZJI0eOhNS3NmuQDAL/4isHRpKZbuit5eYMeO0uPp6dLVg026uko9tpPHZHZW\nUq2WLbNvV6A8bW1yJRe/ugs56E1P83rot94K3H+/aysuLf+3XVSkUCKQzJoASlWioezfL5LVokHQ\nm55Ugk5ENxPRLiLaTUTvr/K6f05ETETb9JnY4PT2AseOye30aUkRdCHo1XLRQ1GRnyTnPYKgNz01\nBZ2IWgHcD+AWAFsA3ElEW8q8bjmA9wL4oW4jG5p4cZG6fHbpoVfq4BcE3T8qCXqY62ha0njo1wPY\nzczjzHwGwIMAbi/zuj8E8B8BnNJoX+MTLy5ykYOuqOahh8ZcftLdLSfb8+fl8cyMzHOEuY6mJY2g\n9wJ4JfZ4Ktr2Y4hoK4A+Zv5fGm1rDuLFRVNTF2+zydq1l2ZNANI47MCB4KH7SHd3qS0DEHLQA8Un\nRYmoBcBHAPxuitfeRUTbiWj7nPoSNjvxkItLD10tFp300OfmpMQ8eOj+kawWDYLe9KQR9GkA8Ty6\n9dE2xXIAVwH4DhHtAfAmAA+Vmxhl5geYeRszb+vs7MxvdSOxdKk07FEhlyVLgMsuc2NLuVz0UFTk\nL8l5jyDoTU+aPPSnAGwkokGIkN8B4F+qJ5n5CIA16jERfQfAv2fm7XpNbWBUcVFbW2kRaxeUqxYN\ngu4vQdADCWp66Mx8DsDdAB4BMAbgi8y8g4juI6LbTBvYFKhcdFc56IpyHnqoEvUXNSb79gHHj0tb\n1jBOTU2qSlFmfhjAw4lt91R47U3FzWoyenuBRx8VD/2GG9zZEV8sWl0lBA/dX9rbgdWrZf4l5KAH\n0Myl/z7R2ys/yNZWtx56d3dpsWi1zursrMT5Qyqcn6gwWRD0AJq59N8nenok/ezMGfchF+DisMvM\nTPDOfSYIeiBGEHQfiIu4aw8duHhiNFSJ+k0Q9ECMIOg+4IugV/LQw0Sbv8QFfcGCUqgs0JQEQfcB\nXwQ9eOj1R0+P9EDfsSN0xAwEQfeCdetk9aKWFrfe8PLl0vdceehnzwIHDwZB9xl1Eh4ZCeGWQBB0\nL2htFSFft04um11BdHEueij795/4VVUQ9KYnpC36wvr1siaka+LVoiEH3X/iIh4EvekJgu4LH/6w\nawuEri5g5075O1SJ+k9cxMM4NT1B0H3hrW91bYHQ3Q08/rj8HTx0/1myRJq7HTkSPPRAiKEHEnR1\nSaXo6dNB0OsFJeRB0JueIOiBi4nnoqsVcJYudWtToDpB0AMRQdADF6NEYWYm5KDXC0HQAxFB0AMX\nk/TQw0Sb/6xfL6mva9e6tiTgmCDogYuJ5zUHD70++O3fBr7+dbc1DAEvCFkugYvp7JQCIxVyufFG\n1xYFatHdHcItAQDBQw8kUYtFT05K2X8IuQQCdUMQ9MCldHcDzz0nf4eQSyBQNwRBD1xKVxfwwgul\nvwOBQF0QBD1wKd3dUlgEBA89EKgjUgk6Ed1MRLuIaDcRvb/M8/+WiJ4nomeI6Aki2qLf1IA14l55\nEPRAoG6oKehE1ArgfgC3ANgC4M4ygv0/mflqZr4OwH8C8BHtlgbsEQQ9EKhL0njo1wPYzczjzHwG\nwIMAbo+/gJmPxh4uBcD6TAxYR6XALV8uzZ8CgUBdkCYPvRfAK7HHUwDemHwREf07AL8DYCGAt2ix\nLuAG5aGHCdFAoK7QNinKzPcz8+UAfg/AB8u9hojuIqLtRLR9bm5O10cHdKM89BBuCQTqijSCPg2g\nL/Z4fbStEg8CeHu5J5j5AWbexszbOjs701sZsIvyzIOgBwJ1RRpBfwrARiIaJKKFAO4A8FD8BUS0\nMfbwVgAv6zMxYJ3ly4EVK6TpUyAQqBtqxtCZ+RwR3Q3gEQCtAD7DzDuI6D4A25n5IQB3E9HbAJwF\ncAjAr5o0OmAYIuBb3wI2bHBtSSAQyAAxu0lI2bZtG2/fvt3JZwcCgUC9QkRPM/O2cs+FStFAIBBo\nEIKgBwKBQIMQBD0QCAQahCDogUAg0CAEQQ8EAoEGIQh6IBAINAhB0AOBQKBBCIIeCAQCDYKzwiIi\nmgOwN+fb1wA4oMGMsJ+wn0awJeynufazgZnLNsNyJuhFIKLtlSqlwn7Cfmzsxydbwn6adz9JQsgl\nEAgEGoQg6IFAINAg1KugPxD2E/bjeD8+2RL207z7uYi6jKEHAoFA4FLq1UMPBAKBQIIg6IFAINAg\nBEEPBAKBBqHmEnS+QESDAH4LwABidjPzbTn2dRWALQAWxfbz3zPu4zIAv1LGnvfksGclgI0Je75X\nr/b4ZEu0j1bIWrdJez6S1Z5of2sT9kzWuT1axiuMlXl7alE3gg7g7wB8GsDXAFzIuxMiuhfATRBB\nfxjALQCeAJBJ0KP3/gDA8wXteTeA9wJYD+AZAG8C8I8A3lLH9vhkCyDfmVMa7LkNwH8B0ANgP4AN\nAMYAvL7O7Sk8XmGsrNlTHWauixuAH2raz/OQUNOz0eN1AL6dYz8jGu1ZBOCZ6PEmAF+uZ3t8siV6\n33Oa7HkWwGoAo9HjNwP4dAPYU3i8wljZsafWrZ5i6B8jonuJ6CeJaKu65djPPDNfAHCOiFZAzpZ9\nOfbzWSL6TSLqJqJV6pZjP6eY+RQAEFE7M+8EcGWd2+OTLQDwDSL6Zznel+QsMx8E0EJELcz8OIA8\n5du+2aNjvMJY2bGnKvUUcrkawLsgl3Dq0oeR/ZJuexQz/EsATwM4Drk0zMoZAH8G4A8iO5Q9Qxn3\nMxXZ83cAvk1Eh5CvaZlP9vhkCyDhhK8QUQuAswAIADPzioz7OUxEywB8D8DniGg/gBMNYI+O8Qpj\nZceeqtRNYRER7QawhZnPaNznAIAVzPxcjveOA7iemXV0XlP7vBFAB4BvMPPZerXHJ1ui904AuB3A\n81zgC09ESwHMQ0J2/yqy538w82t1bo/W8QpjZc6emuiO4Zi6Qc78azXs59E021Ls51sAlmiw57Np\nttWTPT7ZEr3newBaNNjzH9Nsq0N7Co9XGCs79tS61VPI5TIAO4noKQCn1UZOmbZIRIsALAGwJkqv\nouipFQB6c9hzAsAzRPR4wp6sqXkXzXJHaVJvqHN7fLIFAMYBfIeIvpGwJ2vq2T8F8HuJbbeU2VZv\n9ugYrzBWduypSj0J+r0F3/9vAPw2JG1oJLb9KIBP5tjf30W3XBDRBwD8PoDFRHRUbYbEM/M07vHJ\nHp9sAYCJ6LYwumW15/8A8H8CGCKieHhuOYDvN4A9uccrjJV1e6p/XuT6Nw1E9FvM/AnXdiiI6E+Y\n+QOu7VD4ZI8vthBRB4CVAP4EwPtjTx1j3THQOrQnsimMlQf21I2gE9GbAHwCwGbImbIVwAlOOdtM\nRO+o9jwzfzmjPRshg5SsOE2VGVAr5ZKZR6o977M9PtkS7a8TwH+AhAXi9qTKkKqVwpf1h+mhPbnH\nK4yVXXtqUU8hl08CuAPA30DyN38FwBUZ3v/zVZ5jAJkEHcD/BwkDfRRSJPDryNYb57/UsCdrOqZP\n9vhkCwB8DsAXAPwcgH8L4FcBzGV4/9MopfNR4rk86Zi+2VNkvMJY2bWnOrpnWU3dAGyP7p+LbRt1\naM/T0f3zyW3Nbo9PtiTsiX93ngr2+DdeHh8bL+ypdaunStGTRLQQMhv/n4jofcjRLZKI1hHRp6NZ\naxDRFiL6jRz2nI6KDV4moruJ6BcALMthzxIi+iARPRA93khEP1fn9vhkCyAFIQCwj4huJaJhAJkr\nV0n4ZSL6v6PH/UR0/I3feQAAD5tJREFUfQPYU3i8wlhZs6c6rs8oGc6UGwAshqQZ3gvgIwBel2M/\n3wDwSyj1cmlDzDPJsJ+fgHzp10MuWb8M4E059vMFSIzuhejxEkT9MOrVHp9sid73c5BCjqsAPA65\nDL4tx37+HMD9AMaixyuRw1vz0J7C4xXGyo49NT9H9w59v6mDiFi4Js8XT6M9KpQUt+fZYI9ftkSf\nPRLsCWPlsz3eT4oS0fMoTSpcAjNfk3GXJ4hotdpnlD1zJIM9X6thT9b+7GeIaHHMnssRK2CoJ3t8\nsiV6/Sdq2JO10OksScGMsqcTGVqqemiPzvEKY2XQnrR4L+iQSx4A+HfR/Wej+19GlQNehd8B8BCA\ny4no+wA6AfyLDO//z9H9OwB0Afgf0eM7AczmsOdeAN8E0EdEnwNwA4Bfq1N7fLIFALZH9zdAUvK+\nED3+RQAv5rDn4wC+AmAtEf0x5HvzwTq2R+d4hbEya08q6ikPfZSZhxPbRpg5cwtdImqDtPYkALs4\nYwOhaB/bmXlbrW0p97UasiAAAfgB52iS5JM9PtkS7eMHAP4JM5+LHi8A8L+Z+U059rUJwFsjex5l\n5rEGsEfLeIWxMm9PLerBQ1cQEd3AzN+PHvwUMmS5UOXCoiuICJyxsAjAUiIaYubxaP+DAJZmsCd5\nItoX3fcTUT9nLMjwzB6fbAFkAmoFAFXEsSzaltaeeFbDfgCfjz/H2YtDfLMn93iFsbJuT1XqSdB/\nA8BnSEppCcAhAP86w/tVYdFaAD8F4LHo8ZsB/AOyFxa9D9K0ZzyyZwOkX0xaVEHGIkih1LPRfq6B\nXO79ZB3b45MtAPCnAEZJmk8RgJ8G8KEM71fFIQSgH/LdI0jDuEkAg3VuT5HxCmNl157quJr1LTBb\n3AGgo8D7vwWgO/a4G8AjOffVDuDa6Naecx9fBnB17PFVAL5U7/b4ZEv03i5IX+vbAXTl3MdfAvjZ\n2ONbAPy3BrGn0HiFsbJnT7Vb3cTQAYCIbsWlPRXuy7iPMWbeHHvcAmBHfFuGfV2FS/tfZFpsmoh2\nMHOy9egl2+rNHp9sid6nY0X655n56lrb6tSeQuMVxsqePdWom5ALEf0FpFjhzQA+BZklfjLHrh4l\nokdQimW9E8Df57DnXgA3QX4ED0POuE8AyCRaAJ4jok+hlGHwrwDkWUHJG3t8siWyR9eK9K8S0QcT\n9rzaAPboGK8wVhbsqYlul9/UDVEvhdj9Mshsc559vQPSiOijAH4h5z6eh0zKqorTdQC+nWM/iyAx\nzK9Et/cBWFTP9vhkS8weHSvSrwLwMQCj0e1jAFY1iD2FxiuMlR17an6O7h2augH4YXT/A8giFe0A\ndju058no/mnILDgB2Bns8cuWyA5VHfwMovgwJMwW7PFsvDw8Nl7ZU+tWNyEXAF8nWVX8zyArDjFk\noiETVLCveoztkT1/CfkhHIdcimW1p1DvcE/t8ckWQNOK9FSwN7bH9hQerzBW1uypjuszSs6zZjty\nZrpAUqleB7nsaYX0fv6TgvYMALgm53ufgBQbPAdJF/sQgPsaxR6fbIn2eSOA2wAszPHeb0HSZ8ei\n/XwGBRf69dCeXOMVxsq+PWU/R/cObdwAPFDgvdr7qgP4UIH3au9F7ZM9PtkSvf8uDfZo643toT25\nxiuMlX17yt3qqR96nMwl5DG09FVPkLXpVBwtvcM9tscnWwBZdSYvWnpje25P3vEKY2XfnkvRfYaw\ncQPwzQLv3QANfdUT+8zt4UNT73Bf7fHJFg32aOmN3Yj2hLGyb0+5W10VFvkKEbUws/ZWmHnxyR6f\nbAEAIlrPzFOu7VB4aI834+XhsfHKnnJ4L+gkPYTfDTnzf5Oj5lzRcx9k5j9KuR8tfdWJaAmAu6N9\nfQKycPU7AOyETAIdT7kf3b3D4/t+iZmzLKCtzR4i+hnIWD3KzHti2/81M3/Gsi0EaXfKAL4EKQa5\nHTJWf5FWuEh/b+z4vh/jjJkOOu0pOl5hrNzZU456SFv8b5AK0ScBfJyIvsvMvxM99w4AqQQd+vqq\n/xWAVyBhm/8FmbX+M0js8c8BvCvlfrT0oiaiY7h0VfElajunT8csbA8RfRjAP4Gklf4+Ef2/zPyJ\n6Om7ITP7VmyJuB/SjG0hRBzaIb3wb4W0T35vyv1o6Y1NRMnKSYJ0+3wOyLRYiy57dIxXGCsL9qRG\ndwxH9w0Xzwq3AXgAEp9rR46YVrn3IFoeKuX7VcUYAZhB6SqH4rZm2N/2NNuqvP/jkBLtdbFtEwWO\nd257IFV1bdHfl0HKyD9a6bhbODbPR/cLABxElG4WfY/yjNUP1P8X2+8PMrz/IYjgbYLM5QxAnIMN\nADY4sEfbeIWxMmtP2ls9ZLksVH8w8zlmvgtStfUY8s2iExHdEHuQqa96zBYG8HB0rx7niV8tJaIf\nF19Qxt7hLJdsHwPweSJ6T5RpUCSOVsSeNo4WAmDmw5CWxSuI6G8QG0dLtgCAsuUsJEXsTPT4HPIt\n/6V6Yysy9cZmCT/8LcQpuZYlxHGWmfcyc+ZilaL2QO94hbEyaE9qdJ8hdN8gZ8mby2x/N+QAZ93f\nGyA9m/dAKr6eAbA1w/s/BWBZme2XA3gihz03Q/oifwfAdyO7fibHfloAvAfA/wbwaoHjndseAF8H\ncGOZ7X8E4ILtYwPgGxXGqgtRuXtGe349+s78FYC/BjAB4Fdz7GcpJLvqqwCmCoxVIXt0jlcYKzv2\n1Lp5PylqCpKFMsDMqReITrFP4hwHlIjaIZd2gPTQSL24bpl9dQMYZuaHC+wjlz0kiwSDmefLPNfL\nzNO2bKmxz6UAljLz/hzv7QLwxujhD5l5poAd1wL4SWb+iwL7yG2P7vEKY2XPnkrUQ8jlEojogYLv\nvxWyIst7iegeIrpHhz15xDxiI2Ti51oA7ySiX8lrCzPvA/D2vO8vYg8zzyfFgYg+FD2XWcyL2FIJ\nIvoQM5/IIxARpyHLrB2CTJL9dF5bmPlZiAdahNz2GBivMFaW7KmIbpffxg0ZJjHLvPcvIJOIr0AK\ni54H8GmH9twLKTSYhRRkzCDnSi8+2uOTLRrseXf0fTkU2TUP4LFgTxgrF/aUu9Wlhw5ZbDUvP8XM\nvwLgEDP/P5A1DzPlbGu2519AmhrNMPOvQ7ybjgayh2q/xJotRe15L6Qici8zvxnAMIDDwR4AYaxc\n2HMJdSnozHxzgberS8yTRNQD6bHQ7dIelqKJc0S0AiLGfQ1kzxs8sqWoPaeY+RQg8WJm3gkJMQR7\nwli5sOcSvC8siiYvPwCJC6+FpOTth8w6/ylLulUWCvVVN2BPoV7UPtlDRG2QFqG/AFmEBACmieir\nkLDW2Ypv1myLIXsK9cZucHvCWBm0Jy3eZ7mQrP/5GIC/5mhWOJot/lUAb2Xmf1Zg3+2QZbJSZ7oY\ntmcAwApmTr0Wo0/2ENHnIZeRfw1A9bxYH9myipnfacsWC/bcCAkpfJOjnOlgz4/3O4AwVlrtSY3u\noLzuG4BdeZ5Lue/MfdUN2/OherYHwEt5njN4bEzak7k3drPYE8ZKvz1pb/UQQ99LRP+BiNapDUS0\njoh+D5KpUoQ8fdVN2pOnIZdP9rxGRL8YVasqW1qI6J2Q2X2btpi2J09v7GaxJ4yVfntSUQ+C/k4A\nqwF8l4gOEdFrkGq0VQB+qeC+82SDmLQnzyy6T/bcAcl2mCGil4joJUj62jui52zaEuxJb89sZM/L\nmuzxyZZGsSfdjqNLAK8hok2Q+NUPONaelohuZuZvNoo9lLMXtU/2ENEbIROzP4JUDf4kgBe5QOVq\nXlsM25OrN7Zv9sTevzr682PM/MsFbSnUU12nLdH+vDk2Ouypum/fBZ2I3gNpeTsG4DoA72Xmr0bP\njTDz1pT70dVXXZc9uvqqa7Gnwr4z9VUnonsB3ALJnvo2gOshVwv/FMAjzPzHGfalo6+6FnuItPXq\n1nZ8yuw7T6/uh8psfgtkkh1stwe+Llt0jZUWeyrsO/NYpd53HQj685AeCsej2fMvAfgsM3+MiEaZ\neTjlfj6FUl/1dwH4cV/1jEKsy54votRX/UqIIH8BEn/sYuZUfdU12lO2rzqAk0jZVz2y5TpIa+MZ\nAOuZ+ShJz5AfcvpFROJ9un8ewI/7dOcYKx32/FeUenUfxcW9umeZOVWvbo32lO3VDWAXkGmxlhFI\nT+5PQcaeAHweUUiBmb+bYh+6xqqwLdF+dI2VLnu0jFVqTM226roB2JF4vAzANyEd0J7JsB8tfdU1\n2qOlr7pGewr3VY8fx+QxzWiLlj7dOu2J7gv16tZoj5Ze3ZA5tPdBrhaui7aNZxxzXWNV2BbNY6XL\nHq191Wvd6mFSdJaIrlMPWEIRPwdgDYCrM+xHV191Xfao9zOK9VXXYg/r6at+JgolAbGqOpLipywx\nVV19unXZo6tXtxZ7WFOvbma+wMwfhbR2/QMi+iSyFxtqGStNtgCaxkqXPbrGKssHen2DxOa6Kjx3\nQ4b9aOmrrtEeLX3VddkTe0/uvuoA2itsXwPg6gz70dKnW6M9Wnp167In9j4tvbpj+7sVwIczvkdr\nD/witugcK132mBqrSjfvY+jNCFG+vuqabSjcV73AZ2vvq24CKtCrW7MdhXt1F/jsMFbZ7DA6VvUQ\ncjEGFeyrrhsq3lddG6ynr3rezzbRV10rVLxXtzZYT6/uvJ8dxioDpseqqQUd+SpFTRLsqUzuNDFD\nBHsq45MtQBPZ0+yC7vyMnSDYUxlj1XU5CfZUxidbgCayJ8TQA3VB0epD3QR7KuOTLUBz2dM0gk76\n+4YHe8zZUrYXdWRLnl7UwZ4msCXY01yCbqxveLBHuy3GelEHexrXlmBPcwn6LmYuu+RTteeCPfbt\nqdZDJmt/mWBP89gS7GmuSdG9ZK5veLBHLyZ7UQd7GteWprenmQTdZN/wYI9eTPYND/aYscVE3/Bg\nT0aaJuQCANQkfdUbwR4y1Dc82GPUJq19w4vSjPY0jaCTwb7hwR7tthjrGx7s0W6Lsb7hwZ4cmGoS\n49sN0uZzWfT3AIDtENECMrT5DPZYs6UV0o/9KGQFeUB6x6dugRrssWLLCKTx3U0Abozu90V/3+jg\n2DS1Pc0UQ2/hKIzA0sLyJgC3ENFH4KaSLNhTmXPMfJ6ZTwL4ETMfjeyaR7Z2tcEe82wD8DSAPwBw\nhJm/A2Cemb/LKReBCPboo5kEXWsf82CPUXT1MQ/2GIb19TEP9migmWLo6yGezUyZ527g2BqjwR63\n9hBROzOfLrN9DYBuZn7eli3BnmwQ0a2QPvy/78qGOM1mT9MIeiAQCDQ6zRRyCQQCgYYmCHogEAg0\nCEHQA4FAoEEIgh4IBAINQhD0QCAQaBD+f8v/dD3pjpVPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93TiNLC348U3",
        "colab_type": "text"
      },
      "source": [
        "Each of these innermost pairs slopes downwards, so we can say that\n",
        "Adam consistently outperformed Adadelta.\n",
        "\n",
        "The general trend in each second-level group is also downwards, so\n",
        "adding more layers usually caused a loss of performance.\n",
        "\n",
        "The general trend in the largest groups is going upwards, suggesting\n",
        "that more neurons are better than fewer.\n",
        "\n",
        "It’s interesting to note that the worst performance by far was the result\n",
        "of 4 dense-dropout layers of 20 neurons each. So that’s a structure to\n",
        "avoid for this data.\n",
        "\n",
        "Keep in mind that we’re always referring to “layers” as our combination\n",
        "of dense and dropout layers, using our default dropout rate of 0.2.\n",
        "\n",
        "There’s no hard and fast rule for making these choices. We need to use\n",
        "our judgment based on our knowledge of our model and data, coupled\n",
        "with the results of our experiments, to guide our search strategy. If\n",
        "we search with too fine a grid we can waste a lot of time, but if we use\n",
        "too coarse a grid we could miss a big spike in performance. Generally\n",
        "speaking, searching for performance is a task that rewards both intuition\n",
        "and analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1yLFRDJ4tO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "outputId": "0db996cf-6df7-4e45-cba0-ee9e3930fb1d"
      },
      "source": [
        "from tensorflow.keras import backend\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "param_grid2 = dict(model_step__number_of_layers=[1, 2], model_step__neurons_per_layer=[50, 80, 110, 170])\n",
        "\n",
        "grid_searcher2 = GridSearchCV(estimator=pipeline, param_grid=param_grid2, verbose=2)\n",
        "search_results2 = grid_searcher2.fit(X_train, original_y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=1, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=1, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=1, total= 1.1min\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=2, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=2, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=50, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=50, model_step__number_of_layers=2, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=1, total= 1.3min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=1, total= 1.3min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=1, total= 1.2min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=2, total= 1.5min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=2, total= 1.5min\n",
            "[CV] model_step__neurons_per_layer=80, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=80, model_step__number_of_layers=2, total= 1.4min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=1, total= 1.5min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=1, total= 1.5min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=1, total= 1.5min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=2, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=2, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=110, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=110, model_step__number_of_layers=2, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=1, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=1, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=1, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=2, total= 2.4min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=2, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=170, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=170, model_step__number_of_layers=2, total= 2.5min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 37.9min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiqQCWP06H_r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "eac8ba68-9e8a-4323-edd9-201873f9be0b"
      },
      "source": [
        "best_index2 = np.argmax(search_results2.cv_results_['mean_test_score'])\n",
        "print(f'best set of parameters:\\n index {str(best_index2)}\\n {str(search_results2.cv_results_[\"params\"][best_index2])}\\n′.')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best set of parameters:\n",
            " index 7\n",
            " {'model_step__neurons_per_layer': 170, 'model_step__number_of_layers': 2}\n",
            "′.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ScOMxN-6Je-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "4a98ff4a-6fae-46dc-a9c9-542fda689707"
      },
      "source": [
        "params2 = search_results2.cv_results_['params']\n",
        "dict_vals = [params2[i].values() for i in range(len(params2))]\n",
        "name_list =[[str(v) for v in dv] for dv in dict_vals]\n",
        "xlabels = ['-'.join(name_list[i]) for i in range(len(name_list))]\n",
        "\n",
        "plt.plot(search_results2.cv_results_['mean_test_score'], 'r')\n",
        "plt.xticks(np.arange(len(xlabels)), xlabels, rotation='vertical')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAELCAYAAADQsFGkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZQU1fnG8e/LqgZEWYIiChrBiBEx\nTnBLRCFRMFFQQcEE0Yho4vIzxjUaYzAuUROXxGVAUXEBFaPigmIUjLsMsqggi0jYFCcCRgSEYd7f\nH7fGtG0P9Aw9U13dz+ecPqe7qrr66VHq7br31i1zd0REpPg0iDuAiIjEQwVARKRIqQCIiBQpFQAR\nkSKlAiAiUqQaxR2gJlq3bu0dO3aMO4aISKJMnTr1P+7eJn15ogpAx44dKSsrizuGiEiimNm/My1X\nE5CISJFSARARKVIqACIiRUoFQESkSKkAiIgUKRUAEZEipQIgIlKkVABERPLZ7Nlw7rlQUZHzXasA\niIjko3ffhYEDYa+9YORImDEj5x+hAiAikk9mzoT+/WHvveHpp+Gii2DhQthvv5x/VKKmghARKVjT\npsHw4fD447DttnDZZaHpp1WrOvtIFQARkThNmQJXXglPPgktWsAf/gD/93+w/fZ1/tEqACIicXjj\njfCLf8KEcLC/8ko4++xQBOqJCoCISH169dVw4J84MTTvXH01nHlmaPapZyoAIiL14V//gj/+EV58\nEdq0gT//GX79a2jWLLZIWY0CMrPeZjbHzOab2cUZ1ncwsxfMbKaZTTaz9tHyw8xsespjnZn1i9b1\nMrO3o+WvmNnuuf1qIiIxc4dJk+DQQ6FHD3jvPfjLX+DDD+HCC2M9+EMWBcDMGgK3An2ALsAgM+uS\nttkNwGh37woMB64BcPdJ7t7N3bsBPYE1wMToPbcDP4/WPQhcloPvIyISP3d4/nk45BDo2RPmzoWb\nbgoH/vPOg299K+6EQHZnAN2B+e6+wN3XA2OBvmnbdAFejJ5PyrAeoD8wwd3XRK8dqGr0agEsq0lw\nEZG84w7PPgsHHwyHHx4O+H//OyxYEEb2bL113Am/JpsCsBOwOOX1kmhZqhnAsdHzY4DmZpY+eHUg\nMCbl9VDgGTNbAgwGrs304WY2zMzKzKysvLw8i7giIvXMHZ56CvbfH/r0gaVL4fbb4YMPQgfvVlvF\nnTCjXF0JfD7Qw8ymAT2ApcDGqpVmtiOwN/Bcynt+Axzp7u2Bu4G/Ztqxu49w9xJ3L2nT5hv3NBYR\niY87PPEElJTAUUdBeTmMGAHz5sEZZ0DTpnEn3KRsRgEtBXZOed0+WvYVd19GdAZgZs2A49x9Vcom\nxwOPufuGaJs2wD7u/ma0/iHg2Vp9AxGR+lZZCY89Fsbuz5gB3/kOjBoFv/gFNG4cd7qsZXMGMAXo\nZGa7mlkTQlPO+NQNzKy1mVXt6xJgVNo+BvH15p+VQAsz6xy9/gkwu6bhRUTqVWUlPPww7LNPmK9n\nzRq49154/3045ZREHfwhizMAd68ws7MIzTcNgVHu/p6ZDQfK3H08cChwjZk58C/gzKr3m1lHwhnE\nS2n7PA141MwqCQXhl7n6UiIiObVxYzjw/+lPMGsWfPe78MADcMIJ0LBh3Olqzdw97gxZKykp8bKy\nsrhjiEixqKiAsWPDgX/OnDA18+9/H379J+jAb2ZT3b0kfbmmgxYRSbdhA9xzD+y5JwweHDpzx40L\nUzUn/Fd/Kk0FISJSZf16uO++MD/PggWw776hs/foo6FB4f1eLrxvJCJSU19+CaWl0LkzDB0KLVvC\n+PEwdSr061eQB39QARCRYrZuHdx2G3TqFMbt77BDuAvXW2+Fcf1mcSesU2oCEpHis3Yt3HknXHst\nLFsGBx0UXv/kJwV/0E+lAiAixeXtt2HAgNDG/6MfwejRYcK2IjrwV1ETkIgUB/fQzn/QQaGz95//\nDHP09+pVlAd/UAEQkWKwenUYznnGGWFu/mnTwoG/yKkAiEhhmzULuneHMWPC3D3PPAOtW8edKi+o\nD0BECtf998Ppp4c7bz3/fGjrl6/oDEBECs+6deHAP3hwmKp52jQd/DNQARCRwvLBB6Gjd8QIuOgi\neOEFaNcu7lR5SU1AIlI4HnssTMvcoAE8+ST87GdxJ8prOgMQkeTbsAF++1s49tgwncPbb+vgnwWd\nAYhIsi1ZEmbofO01OOssuOGGvL8VY75QARCR5Jo4EX7+89DpO3ZsKASSNTUBiUjybNwIl18OvXvD\njjuGWTt18K8xnQGISLJ88gmceGIY3XPKKfD3v8M228SdKpFUAEQkOV5+OfzSX7kSRo0KBUBqTU1A\nIpL/KivhuuvgsMPCVb1vvqmDfw7oDEBE8tvKlTBkSBjXP2BAmLd/223jTlUQVABEJH+VlYWD/tKl\n8Le/wZlnFu3UzXVBTUAikn/cw60aDz44NP+88koY46+Df06pAIjko/ffhx49YO+9wxTG8+bFnaj+\nfP55GOVz5pnhFo3TpoXpnCXnVABE8ol7GNa4777w3nuw3XZhvHvnzmFWy7/8BRYvjjtl3Xn3XfjB\nD+Dhh+Gaa2D8eGjZMu5UBUsFQCRfLFsGffrA2WeHu1a9804Y9rhoUZjewAzOPx922SXcy/a228KY\n+EJx773hl/5nn8GLL8LFF4dJ3aTO6K8rkg/GjQvNPf/6F9x6a7hr1Y47hnU77xwmOpsyBebODU1C\nK1aEJpJ27eCII+Duu2HVqni/Q22tXQtDh8LJJ8MBB4Qmnx494k5VFLIqAGbW28zmmNl8M7s4w/oO\nZvaCmc00s8lm1j5afpiZTU95rDOzftE6M7OrzGyumc02s3Ny+9VEEuCzz8IQxwEDYLfdwsHv17+u\nvrOzUye47LLQPDRzZpjvfv58+OUvoW1b6NcPHnoIvviifr9Hbc2bBwceCHfdFb7X88/DDjvEnap4\nuPsmH0BD4ANgN6AJMAPokrbNI8CQ6HlP4L4M+2kJrAC2iV6fAowGGkSvv725LPvtt5+LFIyXXnLv\n0MG9QQP33//eff362u2nstL9zTfdf/Mb93bt3MF9m23cBw50f+IJ93Xrcho7Z8aNc2/e3L1VK/cJ\nE+JOU9CAMs9wTM3mDKA7MN/dF7j7emAs0Ddtmy7Ai9HzSRnWA/QHJrj7muj1r4Dh7l4ZFaICaswU\n2YQvv4QLLwzt/I0awauvwvDh0Lhx7fZnFtrO//rX0F8weTKcdFL4Nd23bzgz+OUvw+uKilx+k9pZ\nvx7OPRf694e99gpnPb17x52qKGVTAHYCUocdLImWpZoBHBs9PwZobmat0rYZCIxJef0d4AQzKzOz\nCWbWKfvYIgn17ruw//5w/fVw2mkwfXpo986Vhg1D+/ntt8NHH8GECaEIjBsHhx8OO+0UxtO/8koY\nX1/fFi2CQw6Bm28OReCll0Ifh8QiV53A5wM9zGwa0ANYCmysWmlmOwJ7A8+lvKcpsM7dS4CRwKhM\nOzazYVGRKCsvL89RXJF6VlkJN94YhnJ+9FEY3lhaGua1qSuNG4df1vfeG0YLPfpoKA533RVGEXXs\nCBdcEO6eFZph69aECWF46+zZoSDdeCM0aVL3nyvVy9Qu5F9vuz8QeC7l9SXAJZvYvhmwJG3Z/wEj\n0pa9D+waPTfgs81lUR+AJNKiRe49e4a2+aOOcl++PN48//2v+/33u//sZ+6NGoVcnTqFfohZs3L/\neRs2uP/ud+Fz9tnHfd683H+GbBJb0AcwBehkZruaWRNCU8741A3MrLWZVe3rEr75a34QX2/+AXgc\nOCx63gOYm0UWkWQZMwa6dg2zV44cCU88Ad/+dryZmjcPd9F68klYvjzk2mUXuOoq6NIF9tkHrr0W\nPvxwyz/r44/D1bxXXx2avF5/HXbffcv3K7mRqSqkP4AjCQfoD4BLo2XDgaOj5/2BedE2dwJNU97b\nkdAk1CBtn9sBTwPvAK8D+2wuh84AJDFWrAijcMD9gAOS8av3o4/cb7nF/cADQ25w339/95tucl+6\ntOb7mzTJfYcdwoik0aNzHleyRzVnAOb10faXIyUlJV5WVhZ3DJFNe+GFMLZ/+XL4wx/CFa2NEjbx\n7sKF4XqCsWNDR7VZGLU0cCAcdxy0Sh/jkaKyEv785zCuv3Pn0N6/1171lVwyMLOpHvpbv0ZXAovk\nytq18JvfwI9/HDp3X389HASTdvCH0EF80UVhiObs2WE+oqVL4fTTw4VaP/0p3HdfmLgt1aefwlFH\nwe9+F+7cNWWKDv55TGcAIrkwfXpoV581K0zRcN11hXefWvfwPceODY9Fi2CrrUIxGDQI2rSBwYND\nu/9NN8EZZ2j65jxR3RmACoDIlti4MUzU9vvfQ+vW4T61xXBRU2UlvPFGKAQPPxyauwB23RUeeQT2\n2y/efPI1KgAiubZwYbji9uWXQ7t4aemm28YLVUVFuKBrxoxwn97tt487kaSprgAksHFSJGbuMHp0\nmLYZwoVWgwcXb3NHo0bQq1d4SKKoE1ikJv7znzBz58knQ7duYUbOk04q3oO/JJoKgEi2nn02zNk/\nfnwY5jhpUhgtI5JQKgAim7NmTRjZ06dPaON/660wm2fDhnEnE9kiKgAimzJlSpjA7Lbbwhj/srLQ\n9CNSAFQARDKpqAi3XjzooHAG8M9/hvn2t9oq7mQiOaNRQCLp5s8Po3reeCNc4HTrrRraKAVJZwAi\nVdzDzJjdusH778ODD4aHDv5SoHQGIALhhilDh4Ypknv2hHvu0Z2qpODpDEDkySfhe9+DiRPDXaqe\nf14HfykKKgBSvFavhmHD4OijoV27MMLn3HOhgf5ZSHHQ/+lSnF5/PbT133lnGNP/5pvhLECkiKgA\nSHHZsCHM3PnDH4ahnpMnh6t6mzaNO5lIvVMBkOLgHu7UddBB8Kc/hWGeM2bAIYfEnUwkNioAUtgq\nKsKc9SUl4U5dixeHWxTecw+0aBF3OpFYqQBIYfriC/jb36BTp3Ax1+rVMGJEmMP/uOPiTieSF3Qd\ngBSWTz4JB/7bboMVK0KTz003hfvUanSPyNeoAEhhmDcP/vKX0LSzfj307QsXXBAKgIhkpAIgyfb6\n63D99fD449CkSbg5y29/C3vsEXcykbynAiDJU1kJTz0VDvyvvALbbQe/+124RWPbtnGnE0kMFQBJ\nji+/hPvvhxtuCJO17bJLaN8/9VRo1izudCKJowIg+W/lSrjjDrjlFvj443AF7wMPhHvzNm4cdzqR\nxFIBkPy1aFH4hT9yZBjGefjhcN990KuXbsIukgMqAJJ/ZswI7ftjx4bXAwfC+efrVowiOZbVwGgz\n621mc8xsvpldnGF9BzN7wcxmmtlkM2sfLT/MzKanPNaZWb+0995iZqtz83UksdzDbRePOCIc6B9/\nPHTqfvBBaPfXwV8k5zZ7BmBmDYFbgZ8AS4ApZjbe3WelbHYDMNrd7zWznsA1wGB3nwR0i/bTEpgP\nTEzZdwmg2y0Vs4oKeOSR8It/2rQwiufqq+GMM3QnLpE6ls0ZQHdgvrsvcPf1wFigb9o2XYAXo+eT\nMqwH6A9McPc18FVhuR64sDbBJeFWr4abb4bdd4cTTww3Xh85MkzVcMklOviL1INsCsBOwOKU10ui\nZalmAMdGz48BmptZq7RtBgJjUl6fBYx394829eFmNszMysysrLy8PIu4kteWL4fLLgtDOM89F9q3\nhyeegFmzwi0Zt9oq7oQiRSNXk6OcD/Qws2lAD2ApsLFqpZntCOwNPBe9bgcMAP62uR27+wh3L3H3\nkjZt2uQortS7OXPC3bc6dAhNPIceCq+9Fi7kOvpozdMjEoNsRgEtBVJvkNo+WvYVd19GdAZgZs2A\n49x9VcomxwOPufuG6PW+wO7AfAvD+bYxs/nuvnutvoXkr9deC+37TzwRpmoYMiRM1dC5c9zJRIpe\nNgVgCtDJzHYlHPgHAiembmBmrYEV7l4JXAKMStvHoGg5AO7+NLBDyvtX6+BfQCorw43Wr78eXn01\ntOdfeimcdZamahDJI5s973b3CkJ7/XPAbOBhd3/PzIab2dHRZocCc8xsLtAWuKrq/WbWkXAG8VJO\nk0v+WbcudOR26QL9+sGSJaGjd9EiuPJKHfxF8oy5e9wZslZSUuJlZWVxx5B0K1fC7beHqRqWL4d9\n9w1TMQ8YAI10raFI3MxsqruXpC/Xv07ZMnfcEa7S/eKLcBHXBRdAz56aqkEkAVQApPY+/zwc8Lt1\ng1tvhX32iTuRiNSAxt5J7Y0ZEy7ouu46HfxFEkgFQGqvtBS+9z048MC4k4hILagASO2UlcHbb4c5\ne9TeL5JIKgBSO6WlsM028ItfxJ1ERGpJBUBq7rPPQvv/wIHQokXcaUSkllQApOYeeCAM+zz99LiT\niMgWUAGQmnEPzT/dusEPfhB3GhHZAioAUjNvvgkzZ6rzV6QAqABIzZSWQrNm4SYuIpJoKgCSvVWr\n4KGHwsG/efO404jIFlIBkOzddx+sXavOX5ECoQIg2XEPE7+VlMD3vx93GhHJAU0GJ9l59dVw396R\nI+NOIiI5ojMAyU5pKWy7bbj4S0QKggqAbN6nn8Ijj4RpH5o1izuNiOSICoBs3ujR8OWX6vwVKTAq\nALJpVVf+HnAAdO0adxoRySF1AsumvfQSzJkDd98ddxIRyTGdAcimlZbCdtvB8cfHnUREckwFQKpX\nXg6PPgonnRTm/heRgqICINW75x7YsEGdvyIFSgVAMqushBEj4Ic/hC5d4k4jInVABUAye/FFmD9f\nv/5FCpgKgGRWWgotW0L//nEnEZE6ogIg3/Txx/D443DyybDVVnGnEZE6ogIg33T33VBRAcOGxZ1E\nROpQVgXAzHqb2Rwzm29mF2dY38HMXjCzmWY22czaR8sPM7PpKY91ZtYvWvdAtM93zWyUmTXO7VeT\nWqnq/D30UNhjj7jTiEgd2mwBMLOGwK1AH6ALMMjM0oeF3ACMdveuwHDgGgB3n+Tu3dy9G9ATWANM\njN7zAPBdYG9ga2Doln8d2WITJ8LCher8FSkC2ZwBdAfmu/sCd18PjAX6pm3TBXgxej4pw3qA/sAE\nd18D4O7PeAR4C2hfmy8gOVZaCm3awDHHxJ1EROpYNgVgJ2Bxyusl0bJUM4Bjo+fHAM3NrFXaNgOB\nMek7j5p+BgPPZvpwMxtmZmVmVlZeXp5FXKm1ZcvgySfhlFOgadO404hIHctVJ/D5QA8zmwb0AJYC\nG6tWmtmOhKae5zK89zbgX+7+cqYdu/sIdy9x95I2bdrkKK5kdNddsHEjnHZa3ElEpB5kMxvoUmDn\nlNfto2VfcfdlRGcAZtYMOM7dV6VscjzwmLtvSH2fmf0BaAOowTluGzeG2z3++Mew++5xpxGRepDN\nGcAUoJOZ7WpmTQhNOeNTNzCz1mZWta9LgFFp+xhEWvOPmQ0FjgAGuXtlbcJLDj37LCxerM5fkSKy\n2QLg7hXAWYTmm9nAw+7+npkNN7Ojo80OBeaY2VygLXBV1fvNrCPhDOKltF3fEW37ejRE9PIt+yqy\nRe64A9q2hb6Z+u9FpBBldUMYd38GeCZt2eUpz8cB46p570K+2WmMu+tmNPli8WJ45hm4+GJorMsx\nRIqFrgQWuPPOcOtHdf6KFBUVgGJXUREKwBFHQMeOcacRkXqkAlDsnn46jP9X569I0VEBKHZ33AHt\n2sHPfhZ3EhGpZyoAxWzhQnjuORg6FBqpT16k2KgAFLORI8EsFAARKToqAMVqwwYYNQqOPBJ23nnz\n24tIwVEBKFbjx4c7f6nzV6RoqQAUq9LS8Mu/T5+4k4hITFQAitH8+fD88+HCr4YN404jIjFRAShG\nI0eGA/+pp8adRERipAJQbNavDzd9P+qoMP5fRIqWCkCxeewxKC9X56+IqAAUndLSMOfP4YfHnURE\nYqYCUEzmzIFJk2DYMGig//QixU5HgWIyYkSY8uGUU+JOIiJ5QAWgWKxbB/fcA/36wQ47xJ1GRPKA\nCkCxePRRWLFCnb8i8hUVgGJRWgrf+Q707Bl3EhHJEyoAxWDWLHj5ZXX+isjX6GhQDEpLw83e1fkr\nIilUAArd2rUwejQcdxy0aRN3GhHJIyoAhe7hh2HVKnX+isg3qAAUutJS2GMP6NEj7iQikmdUAArZ\nzJnw+uuh89cs7jQikmdUAApZaSk0bQpDhsSdRETykApAofriC7j/fhgwAFq1ijuNiOQhFYBCNXYs\n/Pe/6vwVkWplVQDMrLeZzTGz+WZ2cYb1HczsBTObaWaTzax9tPwwM5ue8lhnZv2idbua2ZvRPh8y\nsya5/WpFrrQUunSBgw+OO4mI5KnNFgAzawjcCvQBugCDzKxL2mY3AKPdvSswHLgGwN0nuXs3d+8G\n9ATWABOj9/wZuNHddwdWAro/Ya5MmwZTpoRf/+r8FZFqZHMG0B2Y7+4L3H09MBbom7ZNF+DF6Pmk\nDOsB+gMT3H2NmRmhIIyL1t0L9KtpeKlGaSlsvTUMHhx3EhHJY9kUgJ2AxSmvl0TLUs0Ajo2eHwM0\nN7P0nseBwJjoeStglbtXbGKfAJjZMDMrM7Oy8vLyLOIWuc8/hwcegBNOgO23jzuNiOSxXHUCnw/0\nMLNpQA9gKbCxaqWZ7QjsDTxX0x27+wh3L3H3kjaaymDzHnwQVq9W56+IbFajLLZZCuyc8rp9tOwr\n7r6M6AzAzJoBx7n7qpRNjgcec/cN0etPge3MrFF0FvCNfUotuIfmn65dYf/9404jInkumzOAKUCn\naNROE0JTzvjUDcystZlV7esSYFTaPgbxv+Yf3N0JfQX9o0VDgCdqHl++pqwsdACr81dEsrDZAhD9\nQj+L0HwzG3jY3d8zs+FmdnS02aHAHDObC7QFrqp6v5l1JJxBvJS264uA88xsPqFP4K4t+iYSfv1/\n61vwi1/EnUREEsDCj/FkKCkp8bKysrhj5KfPPoN27eDEE2HkyLjTiEgeMbOp7l6SvlxXAheK+++H\nNWvU+SsiWVMBKARVnb/f/z6UfKPIi4hkpAJQCN54A955R7/+RaRGVAAKQWkpNGsGgwbFnUREEkQF\nIOlWroSHHgojf5o3jzuNiCSICkDSjR4N69ap+UdEakwFIMmqOn+7d4du3eJOIyIJk81UEJKvXnkF\nZs+Gu3QNnYjUXHGcAUydCnPnxp0i90pLYdttw8yfIiI1VBwF4Jxz4HvfgwsvDLdJLASffgrjxsFJ\nJ4XpH0REaqg4CsA//hFujnL99bDHHqHjtLIy7lRb5t574csv1fkrIrVWHAWgbdvQTv7mm9ChAwwZ\nEu6VO2VK3Mlqp6rz96CDwpmNiEgtFEcBqNK9O7z2Wvj1/OGHYc78U0+F5cvjTlYzkyeHPg39+heR\nLVBcBQCgQYPQbj53Lpx/Ptx3H3TuDDfeCBs2bP79+aC0NNzuccCAuJOISIIVXwGosu22cN118O67\noTnovPPCnbQmTow72aZ98kno0xgyJNz4XUSkloq3AFTp3BmeeQaeegoqKuCII6BfP1iwIO5kmd1z\nTzhTGTYs7iQiknAqAFV++tNwNnDttfDPf0KXLnDZZfDFF3En+5/KytD8c8ghsOeecacRkYRTAUjV\ntClcdFHoHzj+eLjqqjBsdMyYMPImbi+8EM5M1PkrIjmgApBJu3bhWoFXX4Uddgi3WTzkEJg+Pd5c\npaXQqhUcd1y8OUSkIKgAbMpBB4VrB0aOhDlzYL/94Fe/gv/8p/6zfPwxPPEEnHxyOFMREdlCKgCb\n07AhDB0amoXOOScUg06d4O9/D53G9WXUqPB56vwVkRxRAcjWdtuFawVmzgz33T37bNh3X5g0qe4/\ne+PGUHh69gyjlkREckAFoKa6dAnXCjz2GKxeHQ7KAwbAv/9dd585cSIsXKjOXxHJKRWA2jAL1wrM\nmgVXXglPPw3f/S788Y+wdm3uP6+0FL797fCZIiI5ogKwJbbeOlwrMGcO9O0LV1wRxuePG5e7YaNL\nl4aL1E45BZo0yc0+RURQAciNnXeGsWPDJG0tWoQmoV69woVlW+quu0IfgDp/RSTHVAByqUePcPex\n226DGTPCfXrPOQdWrqzd/ioqQufv4YfDbrvlNquIFL2sCoCZ9TazOWY238wuzrC+g5m9YGYzzWyy\nmbVPWbeLmU00s9lmNsvMOkbLe5nZ22Y23cxeMbPdc/WlYtWoUbhWYN48OOMMuPXWMGy0tDT8kq+J\nCRNgyRJ1/opIndhsATCzhsCtQB+gCzDIzLqkbXYDMNrduwLDgWtS1o0Grnf3PYHuwCfR8tuBn7t7\nN+BB4LIt+SJ5p2XLcK3AtGnhpi1nnBGGj77ySvb7KC0NVyIfdVTd5RSRopXNGUB3YL67L3D39cBY\noG/aNl2AF6Pnk6rWR4Wikbs/D+Duq919TbSdA9tGz1sAy2r9LfJZ167hWoGHHgr38f3Rj8LUEkuW\nbPp9ixaFM4BTT4XGjesnq4gUlWwKwE7A4pTXS6JlqWYAx0bPjwGam1kroDOwysz+YWbTzOz66IwC\nYCjwjJktAQYD19b2S+Q9szC53Pvvw+WXh/n899gDrr4a1q3L/J477wwjiU47rX6zikjRyFUn8PlA\nDzObBvQAlgIbgUbAj6L1PwB2A06O3vMb4Eh3bw/cDfw1047NbJiZlZlZWXl5eY7ixmSbbcK1ArNn\nQ+/ecOmlsNdeMH7814eNVlSE0T99+oR7GIuI1IFsCsBSYOeU1+2jZV9x92Xufqy77wtcGi1bRThb\nmB41H1UAjwPfN7M2wD7u/ma0i4eAgzJ9uLuPcPcSdy9p06ZNTb5b/tp1V3j00XDfga22CtcQ9OkT\nzhAgjPtftkydvyJSp7IpAFOATma2q5k1AQYC41M3MLPWZla1r0uAUSnv3S464AP0BGYBK4EWZlY1\nsc1PgNm1/xoJ1atXmGL65pvhjTdg773ht7+FW26BnXaCI4+MO6GIFLDNFoDol/tZwHOEg/TD7v6e\nmQ03s6OjzQ4F5pjZXKAtcFX03o2E5p8XzOwdwICR0T5PAx41sxmEPoALcvrNkqJx43CtwLx54Wrf\nG28MncZDh4YhpSIidcQ8H+50laWSkhIvKyuLO0bdmjo13Izm8svDzV9ERLaQmU1195L05fqJmW/2\n2y88RETqmKaCEBEpUioAIkRhK6AAAAc6SURBVCJFSgVARKRIqQCIiBQpFQARkSKlAiAiUqRUAERE\nipQKgIhIkUrUlcBmVg78u5Zvbw38J4dx6lqS8ipr3UlS3iRlhWTl3dKsHdz9G7NpJqoAbAkzK8t0\nKXS+SlJeZa07ScqbpKyQrLx1lVVNQCIiRUoFQESkSBVTARgRd4AaSlJeZa07ScqbpKyQrLx1krVo\n+gBEROTriukMQEREUqgAiIgUKRUAEZEipQIgIlKkiq4AmFle9fybWUMzO93MrjSzg9PWXRZXrtow\ns8vjzpDOzI4ws1PNrGPa8l/Gk0gkfxTkKCAza1ndKmCGu7evzzybYmZ3AtsAbwGDgZfc/bxo3dvu\n/v0489WEmS1y913izlHFzK4Gfgi8DRwF3OTuf4vWJe1v28zdV8edoxAV89+2UM8AyoEyYGrKoyx6\nfDvGXJl0d/cT3f0mYH+gmZn9w8yaEgpWXjGz/1bz+BxoF3e+NEcBPd39XGA/oI+Z3Rity7u/7WbM\nijtAOjPb28zeMLPFZjbCzLZPWfdWnNlqqGj/to1ytaM8swDo5e6L0leY2eIY8mxKk6on7l4BDIua\nUl4EmsWWqnqrgB+4+/L0FXn4t20U/U1x91VmdhQwwsweIeXvni/M7LzqVpGf/y/cDlwBvAEMBV4x\ns6Pd/QOgcZzB0ulvm1mhngHcBGxfzbrr6jNIFsrMrHfqAncfDtwNdIwl0aaNBjpUs+7B+gyShQ/M\nrEfVC3ff6O6nAnOAPeOLVa2rCf/fNk97NCM//602d/dn3X2Vu98AnAU8a2YHAPnWtqy/bQYF2Qcg\nAmBmWwO4+9oM63Zy96X1n6p6ZvYacLa7T82wbrG77xxDrGqZ2QzgEHf/LGVZV+BRoKW7t4otXBr9\nbTPLx8pXJ/Jt9M+mJCkrgJldEXeGTNx9bfrBvyprvh38I6dQ/f0u8nHa4j+Tdibl7jOBXsA/YklU\nPf1tMyiaM4AkjfpIUlZIVt4kZQUwsx3c/eO4c2QrSXmTlBXqJm/RnAEAn8QdoAaSlBWSNaImSVkB\nnok7QA0lKW+SskId5C2aAuDuvTe/VX5IUtbIfnEHqIEkZYXkFawk5U1SVqiDvAVZAMyshZlda2bv\nm9kKM/vUzGZHy7aLO1+qJGUFMLNG0ZXLz5rZTDObCTxtZmeYWb4N/UtM1k0YGXeAGkpS3iRlhTrI\nW5B9AGb2HGEc/b1VbWZmtgMwhHB9wOFx5kuVpKwAZjaGcC3AvcCSaHF7Qt6W7n5CXNnSJSlrFTMz\noDuwU7RoKfCW5+k/1CTlTVJWqJ+8hVoA5rj7HjVdF4ckZQUws7nu3rmm6+KQpKwAZnY4cBswj/CP\nHULB2h34tbtPjCtbJknKm6SsUH95C/VK4H+b2YWEX9XLAcysLXAykG9XqyYpK8AKMxsAPOrulQBm\n1gAYAKyMNdk3JSkrwM3Aj919YepCM9uV0AGYbxevJSlvkrJCPeUtyD4A4ASgFfCSma00sxXAZKAl\ncHycwTJIUlaAgUB/4GMzm2tmc4GPgWOjdfkkSVkh/CBbkmH5UvJsaoVIkvImKSvUU96CPANw95XA\nRdEDM/sRoS3tHXdfEWe2dEnKGllG+AVyJ2GWzd7AwcB7ZP4fNk5JygowCphiZmP539nfzoRidVds\nqaqXpLxJygr1lLdQ+wDecvfu0fOhwJnA48DhwJPufm2c+VIlKSuAmT1A+OGwNfAZ8C3gMcIViubu\nQ2KM9zVJylrFzPYE+vL1jr/x7p53M1ZCsvImKSvUT95CLQDT3H3f6PkU4Eh3LzezbwFvuPve8Sb8\nnyRlBTCzme7e1cwaEf6HbOfuG6MRCzPcvWvMEb+SpKwicSjUPoAGZra9mbUiFLlyAHf/AqiIN9o3\nJCkrhLxNCDMpbgO0iJY3Jf/aUpOUdZPMbELcGWoi3/Ka2dtmdpmZ7RZ3lmzUV96C7AMg/EOfSrhy\nzs1sR3f/yMyakX9X/yUpK4T2x/eBhsClwCNmtgA4ABgbZ7AMkpQVM6tujiIDutVnlmwkLO/2wHbA\nZDP7GBgDPOTuy+KNVa16yVuQTUDVMbNtgLbu/mHcWTYnn7OaWTsAd18WXa38Y2CRu+fdXaASlnUj\n8BKZC/8B7r51PUfapCTltZRJAKOBFoMIo8FmA2PcPa9m4K2vvEVVAETymZm9Cxzj7vMyrMvHOesT\nk9cyzAJrZg2BnwAnuPsp8STLrL7yFmoTkEgSXUH1/XJn12OObF1BcvLOTV/g7huBZ6NHvqmXvIXa\nCSySOO4+zt3nVLO6ulucxiZJed292gv/zCyvfv1D/eVVE5BIApjZInffJe4c2UpS3iRlhdzmVROQ\nSJ6wMF11xlVA2/rMko0k5U1SVqi/vCoAIvmjLXAE35yozoDX6j/OZiUpb5KyQj3lVQEQyR9PAc3c\nfXr6CjObXP9xNitJeZOUFeopr/oARESKlEYBiYgUKRUAEZEipQIgIlKkVABERIrU/wOqEE6QoZDk\ndwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9SP9Jal7XV",
        "colab_type": "text"
      },
      "source": [
        "Let’s crank up both the number of neurons and the search range quite\n",
        "a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDS6y4I_O5gp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "outputId": "65962639-6b12-4200-94d4-797f63ade9b0"
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "param_grid3 = dict(model_step__number_of_layers=[1, 2], model_step__neurons_per_layer=[180, 280, 380, 580])\n",
        "\n",
        "grid_searcher3 = GridSearchCV(estimator=pipeline, param_grid=param_grid3, verbose=2)\n",
        "search_results3 = grid_searcher3.fit(X_train, original_y_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=1, total= 1.8min\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=1 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=1, total= 1.9min\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=1, total= 1.9min\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=2, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=2, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=180, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=180, model_step__number_of_layers=2, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=1, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=1, total= 2.5min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=1, total= 2.4min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=2, total= 3.8min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=2, total= 3.6min\n",
            "[CV] model_step__neurons_per_layer=280, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=280, model_step__number_of_layers=2, total= 3.6min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=1, total= 3.2min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=1, total= 3.2min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=1, total= 3.1min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=2, total= 5.0min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=2, total= 5.0min\n",
            "[CV] model_step__neurons_per_layer=380, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=380, model_step__number_of_layers=2, total= 5.0min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=1, total= 4.4min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=1, total= 4.4min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=1 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=1, total= 4.5min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=2, total= 8.3min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=2, total= 8.2min\n",
            "[CV] model_step__neurons_per_layer=580, model_step__number_of_layers=2 \n",
            "[CV]  model_step__neurons_per_layer=580, model_step__number_of_layers=2, total= 8.2min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 93.8min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKeW3ag7mJpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "673547fe-075e-4362-d7ec-8c981987f7f5"
      },
      "source": [
        "best_index3 = np.argmax(search_results3.cv_results_['mean_test_score'])\n",
        "print(f'best set of parameters:\\n index {str(best_index3)}\\n {str(search_results3.cv_results_[\"params\"][best_index3])}\\n′.')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best set of parameters:\n",
            " index 5\n",
            " {'model_step__neurons_per_layer': 380, 'model_step__number_of_layers': 2}\n",
            "′.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW1dKDiJmQrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "973560b7-7046-4793-e616-383a24678a52"
      },
      "source": [
        "params3 = search_results3.cv_results_['params']\n",
        "dict_vals = [params3[i].values() for i in range(len(params3))]\n",
        "name_list =[[str(v) for v in dv] for dv in dict_vals]\n",
        "xlabels = ['-'.join(name_list[i]) for i in range(len(name_list))]\n",
        "\n",
        "plt.plot(search_results3.cv_results_['mean_test_score'], 'r')\n",
        "plt.xticks(np.arange(len(xlabels)), xlabels, rotation='vertical')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAENCAYAAADkNanAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzjnnSvj/Kxi+UmaqTjoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fyPRGur1AXj",
        "colab_type": "text"
      },
      "source": [
        "We’ll stop searching here, but we could continue to try different values\n",
        "for all of these parameters, or some we didn’t even try (like any\n",
        "of the Normalization object’s parameters, or dropout_ratio in our\n",
        "own model).\n",
        "\n",
        "A good strategy is to start with searches that cover broad ranges with\n",
        "just a few values. When we see where the model is performing best,\n",
        "we can then run another, denser search to explore the area around\n",
        "that zone. This is called multiresolution searching, and it’s just an\n",
        "algorithmic version of what we do when we look for something in the\n",
        "real world.\n",
        "\n",
        "A useful alternative to the exhaustive search performed by the grid\n",
        "searcher is provided by scikit-learn’s RandomizedSearchCV algorithm."
      ]
    }
  ]
}