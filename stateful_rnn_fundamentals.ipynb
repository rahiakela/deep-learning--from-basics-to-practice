{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stateful-rnn-fundamentals.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUSKcV8wGUrrpKb+hjWcxs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/stateful_rnn_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXPewUonB3c",
        "colab_type": "text"
      },
      "source": [
        "# Stateful RNN Fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umw_N_0Onbws",
        "colab_type": "text"
      },
      "source": [
        "We’ve been focusing on one sample at a time, but in practice we usually\n",
        "train in mini-batches. This poses an interesting question for RNNs,\n",
        "since their internal memory is always influenced by previous inputs.\n",
        "When should we clear that memory and let the RNN start over?\n",
        "\n",
        "The usual approach is to clear the internal memory at the start of a\n",
        "new batch, or mini-batch. We don’t clear or reset the weights belonging\n",
        "to the neurons inside the RNN, since those tell it how to do its job.\n",
        "We only clear its changing memory that holds the inputs it’s recently\n",
        "seen. \n",
        "\n",
        "The thinking is that when a new batch begins we’ll possibly be\n",
        "getting data that isn’t a continuation of the most recent samples, so we\n",
        "don’t want to remember stuff from back then.\n",
        "\n",
        "Usually, we shuffle our samples between epochs, so they arrive in an\n",
        "unpredictable order each time. \n",
        "\n",
        "But we can keep their order consistent from epoch to epoch, if we want to. We do this when we call fit() to train our model, setting the optional argument shuffle to False (the default is True).\n",
        "\n",
        "When the data is always arriving in sequence, there’s no reason to reset\n",
        "the memory at the start of each batch, because those samples follow\n",
        "the samples in the previous batch. \n",
        "\n",
        "In other words, the batching just breaks up the grouping of the samples, and not their sequence. In that situation, we can tell Keras to not reset the memory at the start of each batch. This sometimes can help us train a little faster.\n",
        "\n",
        "In Keras, when we take over the responsibility for clearing the memory\n",
        "we say that the RNN is in the stateful mode. In this mode, Keras\n",
        "only resets the internal state when we tell it to. Usually this is at the\n",
        "start (or end) of each epoch.\n",
        "\n",
        "Stateful mode can make training go a little faster, but it comes with\n",
        "limitations. The batch size must be determined in advance, and it\n",
        "becomes a part of the model. The dataset must be a multiple of this\n",
        "batch size.\n",
        "\n",
        "For instance, if the batch size is 100, the dataset must be\n",
        "100, 200, 300, and so on samples long. If it’s 130 or 271 samples, we’ll\n",
        "get an error.\n",
        "\n",
        "When we later give new data to the model for it to evaluate, that data\n",
        "also has to come in batches of the same size we used when we trained.\n",
        "If we want only one prediction, but our batch size is 100, then we can\n",
        "either pad out our one request with 99 more copies of itself, or just\n",
        "load up all the unused entries in the batch with 0’s. We’ll still end up\n",
        "waiting for the network to evaluate all those samples, though.\n",
        "\n",
        "To make a stateful network, we need to do four things.\n",
        "\n",
        "* First, we need to include the optional argument stateful to each RNN\n",
        "(such as an LSTM or GRU) and set it to True. This tells Keras that\n",
        "we’re taking care of when to reset the cell’s state.\n",
        "\n",
        "* Second, we need to include the argument batch_size to the first RNN\n",
        "we make, and set it to the batch size that we’re going to use during\n",
        "training.\n",
        "\n",
        "* Third, when we call fit() we need to set shuffle to False.\n",
        "\n",
        "* Finally, when we want to reset the state, we need to explicitly call\n",
        "reset_states() on our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU4qO05mpKLj",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lptonCfVpLok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "40725a17-a196-4fab-d8e9-321a8526839a"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-L7PFTVunxZ",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gakw0f2uuohY",
        "colab_type": "text"
      },
      "source": [
        "First, let’s generates uniform random data between -amp and +amp and of length xn.\n",
        "\n",
        "Reference: https://github.com/keras-team/keras/blob/master/examples/lstm_stateful.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nv7Yd2fz3bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ----------------------------------------------------------\n",
        "# EDITABLE PARAMETERS\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# length of input\n",
        "input_len = 1000\n",
        "\n",
        "# The window length of the moving average used to generate\n",
        "# the output from the input in the input/output pair used\n",
        "# to train the LSTM\n",
        "# e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],\n",
        "#      then output=[1.5, 2.5, 3.5, 4.5]\n",
        "tsteps = 2\n",
        "\n",
        "# The input sequence length that the LSTM is trained on for each output point\n",
        "lahead = 1\n",
        "\n",
        "# training parameters passed to \"model.fit(...)\"\n",
        "batch_size = 1\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_BlAt14ze7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_uniform_amp(amp=1, xn=10000):\n",
        "    \"\"\"Generates uniform random data between\n",
        "    -amp and +amp\n",
        "    and of length xn\n",
        "    # Arguments\n",
        "        amp: maximum/minimum range of uniform data\n",
        "        xn: length of series\n",
        "    \"\"\"\n",
        "    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\n",
        "    data_input = pd.DataFrame(data_input)\n",
        "    return data_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erfA0-B5zvqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_drop = max(tsteps - 1, lahead - 1)\n",
        "data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\n",
        "\n",
        "# set the target to be a N-point average of the input\n",
        "expected_output = data_input.rolling(window=tsteps, center=False).mean()\n",
        "\n",
        "# when lahead > 1, need to convert the input to \"rolling window view\"\n",
        "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\n",
        "if lahead > 1:\n",
        "    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\n",
        "    data_input = pd.DataFrame(data_input)\n",
        "    for i, c in enumerate(data_input.columns):\n",
        "        data_input[c] = data_input[c].shift(i)\n",
        "\n",
        "# drop the nan\n",
        "expected_output = expected_output[to_drop:]\n",
        "data_input = data_input[to_drop:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa4GYHxTz-RG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "18fd0d12-ae00-4acd-d17e-ee08cf00f982"
      },
      "source": [
        "print('Plotting input and expected output')\n",
        "plt.plot(data_input[0][:10], '.')\n",
        "plt.plot(expected_output[0][:10], '-')\n",
        "plt.legend(['Input', 'Expected output'])\n",
        "plt.title('Input')\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Plotting input and expected output\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkqVSJOEUkqpEv0/Y+lmU5dPBTYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshMUnw7wiq-",
        "colab_type": "text"
      },
      "source": [
        "## Stateful RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0mvHLUqwfgI",
        "colab_type": "text"
      },
      "source": [
        "Now let’s create the stateful RNN. \n",
        "\n",
        "First, we need to set stateful=True when creating every recurrent layer. \n",
        "\n",
        "Second, the stateful RNN needs to know the batch size (since it\n",
        "will preserve a state for each input sequence in the batch), so we must set the batch_input_shape argument in the first layer.\n",
        "\n",
        "Note that we can leave the second dimension unspecified, since the inputs could have any length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVNy4plnwXbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(50, return_sequences=True, stateful=True, input_shape=(lahead, 1), batch_size=batch_size),\n",
        "    keras.layers.LSTM(50, stateful=True),\n",
        "    keras.layers.Dense(1)                             \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE4hReBa0wPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "479b2b15-3ec8-40f2-9b15-d06c661bbbda"
      },
      "source": [
        "# split train/test data\n",
        "def split_data(x, y, ratio=0.8):\n",
        "    to_train = int(input_len * ratio)\n",
        "    # tweak to match with batch_size\n",
        "    to_train -= to_train % batch_size\n",
        "\n",
        "    x_train = x[:to_train]\n",
        "    y_train = y[:to_train]\n",
        "    x_test = x[to_train:]\n",
        "    y_test = y[to_train:]\n",
        "\n",
        "    # tweak to match with batch_size\n",
        "    to_drop = x.shape[0] % batch_size\n",
        "    if to_drop > 0:\n",
        "        x_test = x_test[:-1 * to_drop]\n",
        "        y_test = y_test[:-1 * to_drop]\n",
        "\n",
        "    # some reshaping\n",
        "    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n",
        "    x_train = reshape_3(x_train)\n",
        "    x_test = reshape_3(x_test)\n",
        "\n",
        "    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\n",
        "    y_train = reshape_2(y_train)\n",
        "    y_test = reshape_2(y_test)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\n",
        "print('x_train.shape: ', x_train.shape)\n",
        "print('y_train.shape: ', y_train.shape)\n",
        "print('x_test.shape: ', x_test.shape)\n",
        "print('y_test.shape: ', y_test.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train.shape:  (800, 1, 1)\n",
            "y_train.shape:  (800, 1)\n",
            "x_test.shape:  (200, 1, 1)\n",
            "y_test.shape:  (200, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XYwwCi1yChF",
        "colab_type": "text"
      },
      "source": [
        "To train our model, we need to remember to tell fit() not to shuffle\n",
        "our data. Because we want to reset the state after each epoch, we don’t\n",
        "want to do the usual thing of telling fit() how many epochs to train\n",
        "for, and then walking away, because then the RNN will never be reset.\n",
        "\n",
        "Instead, we’ll tell fit() to train for only 1 epoch, and we’ll put that call\n",
        "in a loop. The loop will repeat for the number of epochs we want to\n",
        "train for. Doing it this way lets us put in a call to reset_states() at\n",
        "the end of each epoch of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSXuHG6xxqzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b403103-deb1-4d43-e87e-878ff6868bf2"
      },
      "source": [
        "model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "for epoch in range(50):\n",
        "  model.fit(x_train, y_train, epochs=1, batch_size=batch_size, shuffle=False, validation_data=(x_test, y_test), verbose=1)\n",
        "  model.reset_states()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 5s 7ms/sample - loss: 5.3910e-04 - val_loss: 3.0246e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.0988e-04 - val_loss: 3.0565e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.2430e-04 - val_loss: 3.1926e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.4284e-04 - val_loss: 3.3060e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 4ms/sample - loss: 3.5289e-04 - val_loss: 3.1926e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 4ms/sample - loss: 3.4639e-04 - val_loss: 2.9675e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 4ms/sample - loss: 3.4605e-04 - val_loss: 2.8359e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.4080e-04 - val_loss: 3.1128e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.3859e-04 - val_loss: 2.9809e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.3502e-04 - val_loss: 3.0874e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.2970e-04 - val_loss: 3.3349e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.3541e-04 - val_loss: 3.4549e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.4542e-04 - val_loss: 3.4161e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.2116e-04 - val_loss: 3.1611e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.7142e-04 - val_loss: 3.1420e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.0658e-04 - val_loss: 2.6742e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.0070e-04 - val_loss: 2.4789e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 3.1151e-04 - val_loss: 2.6193e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 6.6253e-04 - val_loss: 6.7391e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.8561e-04 - val_loss: 8.1521e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.6930e-04 - val_loss: 5.8978e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 0.0011 - val_loss: 0.0015\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.7488e-04 - val_loss: 7.9183e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 6.6053e-04 - val_loss: 1.0129e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.0486e-04 - val_loss: 1.1068e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.3075e-04 - val_loss: 1.5470e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 2.1321e-04 - val_loss: 4.5564e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 2.0355e-04 - val_loss: 1.8025e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.6154e-04 - val_loss: 1.7620e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.4684e-04 - val_loss: 1.5565e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.3803e-04 - val_loss: 1.3300e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.2509e-04 - val_loss: 1.3685e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.2769e-04 - val_loss: 2.0100e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 4ms/sample - loss: 1.1569e-04 - val_loss: 1.4810e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 4ms/sample - loss: 9.3585e-05 - val_loss: 1.3789e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 1.0130e-04 - val_loss: 1.3595e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.7222e-05 - val_loss: 1.1050e-04\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.4141e-05 - val_loss: 7.8921e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 6.3264e-05 - val_loss: 9.8497e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.2972e-05 - val_loss: 8.3556e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.6427e-05 - val_loss: 8.0636e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.0292e-05 - val_loss: 7.5773e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.8641e-05 - val_loss: 7.8586e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.8574e-05 - val_loss: 7.9136e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.8653e-05 - val_loss: 7.8439e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 7.9383e-05 - val_loss: 8.1190e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.0545e-05 - val_loss: 8.5178e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.2053e-05 - val_loss: 8.6992e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.4403e-05 - val_loss: 8.9446e-05\n",
            "Train on 800 samples, validate on 200 samples\n",
            "800/800 [==============================] - 3s 3ms/sample - loss: 8.5355e-05 - val_loss: 9.1609e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}