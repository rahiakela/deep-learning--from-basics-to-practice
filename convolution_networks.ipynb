{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolution-networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/convolution_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9BC6gBzaL0J",
        "colab_type": "text"
      },
      "source": [
        "# Convolution Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grcvrwtzaNCc",
        "colab_type": "text"
      },
      "source": [
        "Let’s build some convolutional neural networks, also called convnets,\n",
        "or more commonly, CNNs.\n",
        "\n",
        "Each convolution layer holds a collection of\n",
        "filters, or kernels, which are rectangles of numbers (often a small\n",
        "square that is 3, 5, or 7 elements on a side). When we use 2D convolution\n",
        "layers with images, each filter in the first layer is applied in turn\n",
        "to every pixel in the input. The output of the filter becomes the value\n",
        "of that element in a new tensor produced at the layer’s output. If there\n",
        "are multiple filters, then the output tensor contains multiple channels,\n",
        "just like the red, green, and blue channels of a color image.\n",
        "\n",
        "We can characterize a convolution layer by\n",
        "the number of dimensions in which the filters are moved. If the filter is\n",
        "moved in just one dimension (for example, down), then we call it a 1D\n",
        "convolution layer. Typically, when we work with images, we slide our\n",
        "filters over the 2D width and height of the tensor, so we usually use 2D\n",
        "convolution layers for image processing. Keras also offers 3D convolution\n",
        "layers for working with volumetric data.\n",
        "\n",
        "In practice we don’t often build and train\n",
        "a new CNN from the ground up. Instead, we usually try to start with\n",
        "an existing network whenever possible, and specialize it for our task\n",
        "by perhaps modifying it, and then training it some more with our own\n",
        "data. Such transfer learning is appealing because we get to start\n",
        "with an existing architecture that is known to work well, and we save\n",
        "the time (sometimes days or weeks) that was invested in training the\n",
        "model we’re building upon. We also get the benefit of the data that\n",
        "network was trained on, which might not be available to us.\n",
        "\n",
        "But it’s important to know how to build our own from scratch. This\n",
        "lets us start fresh when we need to, and gives us the tools to modify an\n",
        "existing network when we want to. Whether we’re working with our\n",
        "own model or one we’ve adopted, knowing what’s going on inside will\n",
        "help us diagnose problems and get the best performance out of our\n",
        "model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ21oxlNb8n4",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7yhl_bfb9-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "keras_backend.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUGjUjK3m4Gs",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the Data for A CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6fTuxl1m5Ga",
        "colab_type": "text"
      },
      "source": [
        "We prepare our MNIST data for a convnet with almost the same process\n",
        "as we’ve been doing so far when the first layer was a Dense, or\n",
        "fully-connected, layer.\n",
        "\n",
        "The difference is in the shape of the feature data. So far, we’ve been\n",
        "shaping our feature data in a 2D grid, with one row per image. Each\n",
        "row held all the pixels for that image.\n",
        "\n",
        "Something important happened when we flattened out our image\n",
        "to make that grid: we lost the spatial information that tells us which\n",
        "pixels are near one another vertically (technically, it’s still there, but\n",
        "definitely not in a structure that’s easily useful). A great thing about\n",
        "CNNs is that they work with inputs as multidimensional tensors, not\n",
        "long 1D lists. For instance, the receptive field for a filter covers a group\n",
        "of spatially-related elements.\n",
        "\n",
        "When working with CNNs there’s no need to flatten out input 2D grids\n",
        "of pixels. We’ll maintain them instead as three-dimensional volumes,\n",
        "where each input image has a height, width, and depth.\n",
        "\n",
        "The MNIST data is black and white, so we have just a single channel of\n",
        "pixel data. But we still have to explicitly tell Keras that we have just that\n",
        "one channel, by making it one of the dimensions of our input tensor.\n",
        "\n",
        "Each image in the input will be reshaped as a 3D block\n",
        "with dimensions 28 by 28 by 1, since we’re using the channels_last\n",
        "convention. Then we stack all 60,000 of these 3D blocks together to\n",
        "make a 4D tensor of shape 60,000 by 1 by 28 by 28, which will serve as\n",
        "input to our CNN.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/image-inputs.PNG?raw=1' width='800'/>\n",
        "\n",
        "It can be easier to think of this not as a 4D\n",
        "structure but instead as a sequence of nested lists: the outermost list\n",
        "contains 60,000 images, each image contains one channel, each channel\n",
        "contains 28 rows, and each row contains 28 elements.\n",
        "\n",
        "Convnets work best with input data scaled from −1 to 1.This means we can’t just divide every pixel by 255. Instead, we’ll use\n",
        "the NumPy function interp() to convert each input value in the range\n",
        "[0,255] to the range [−1,1].\n",
        "\n",
        "```python\n",
        "X_train = np.interp(X_train, [0, 255], [-1, 1])\n",
        "X_test = np.interp(X_test, [0, 255], [-1, 1])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX96Q3f_qmp_",
        "colab_type": "text"
      },
      "source": [
        "Now we’ll re-shape the data into the shape we just discussed. We just\n",
        "tell NumPy how to take our original version of X_train, which was\n",
        "60,000 by 28 by 28, and reshape it into a 4D tensor that’s 60,000 by 1 by 28 by 28. We’re not changing the total number of elements, so\n",
        "NumPy can do this for us.\n",
        "```python\n",
        "X_train = X_train.reshape(X_train.shape[0], image_height, image_width, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_height, image_width, 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ksMSLdlrdPx",
        "colab_type": "text"
      },
      "source": [
        "We’ll place these re-shaping lines right after the scaling step. For completeness,all the pre-processing in one place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc45EYW4rght",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_seed = 42\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRSE_nSFrwo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load MNIST data and save sizes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_height = X_train.shape[1]\n",
        "image_width = X_train.shape[2]\n",
        "number_of_pixels = image_height * image_width\n",
        "\n",
        "# convert to floating-point\n",
        "X_train = keras_backend.cast_to_floatx(X_train)\n",
        "X_test = keras_backend.cast_to_floatx(X_test)\n",
        "\n",
        "# scale data to range [-1, 1]\n",
        "X_train = np.interp(X_train, [0, 255], [-1,1])\n",
        "X_test = np.interp(X_test, [0, 255], [-1,1])\n",
        "\n",
        "# save original y_train and y_test\n",
        "original_y_train = y_train\n",
        "original_y_test = y_test\n",
        "\n",
        "# replace label data with one-hot encoded versions\n",
        "number_of_classes = 1 + max(np.append(y_train, y_test))\n",
        "y_train = to_categorical(y_train, num_classes=number_of_classes)\n",
        "y_test = to_categorical(y_test, num_classes=number_of_classes)\n",
        "\n",
        "# reshape sample data to 4D tensor using channels_last convention \n",
        "X_train = X_train.reshape(X_train.shape[0], image_height, image_width, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_height, image_width, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTLZOerv3Jrb",
        "colab_type": "text"
      },
      "source": [
        "Shaping the feature data into these 4D tensors is a necessary pre-processing\n",
        "step. It puts the data into the structure that is expected by the\n",
        "convolution layer that will sit at the start of our convnet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2p4mmLetfnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A little utility to draw accuracy and loss plots\n",
        "def plot_accuracy_and_loss(history, plot_title, filename):\n",
        "    xs = range(len(history.history['acc']))\n",
        "\n",
        "    plt.figure(figsize=(10,3))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(xs, history.history['acc'], label='train')\n",
        "    plt.plot(xs, history.history['val_acc'], label='validation')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title(plot_title+', Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(xs, history.history['loss'], label='train')\n",
        "    plt.plot(xs, history.history['val_loss'], label='validation')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title(plot_title+', Loss')\n",
        "\n",
        "    #plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5magi5BE3M1N",
        "colab_type": "text"
      },
      "source": [
        "## Convolution Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aPo2AZ63OoS",
        "colab_type": "text"
      },
      "source": [
        "The Conv2D layer takes two unnamed, mandatory arguments at the\n",
        "start of its argument list, followed by a variety of optional arguments.\n",
        "\n",
        "The first mandatory argument is an integer specifying the number of\n",
        "filters the layer should manage.Each filter\n",
        "is applied to the input independently, and produces its own output. So\n",
        "if our input has one channel (as our input does), and we use 5 filters\n",
        "in a convolution layer, the output will have 5 channels.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/image-channel-inputs.PNG?raw=1' width='800'/>\n",
        "\n",
        "The second argument to Conv2D is a list that gives the dimensions of\n",
        "the filters on this layer. if we have\n",
        "5 filters and each is 3 by 3, then these arguments would be 5,[3,3].\n",
        "This tells the layer to automatically allocate and initialize 5 volumes,\n",
        "each of shape 3 by 3 by 1 (the trailing 1 is the number of channels).\n",
        "\n",
        "In practice, we almost always use square kernels, often of 3 or 5 elements\n",
        "on a side. Experience has shown that these sizes, coupled\n",
        "with reduction in the size of the input (either by pooling or convolution\n",
        "striding), represents a good tradeoff of computation and results.\n",
        "\n",
        "Keep in mind that these filter kernels are 3D volumes, since there’s\n",
        "one channel in the kernel for each channel in the input.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/image-channel-inputs-1.PNG?raw=1' width='800'/>\n",
        "\n",
        "Each filter automatically holds as many channels as there\n",
        "are in the input. Here a 5 by 5 filter is being applied to a 3-channel input,\n",
        "so the system automatically gives the filter 3 channels as well. Each of\n",
        "the 75 values in the input (bottom) is multiplied by its corresponding\n",
        "value in the filter (middle), and all of those products are added together\n",
        "to produce a single number (top), the output of that filter for that location\n",
        "of the input.\n",
        "\n",
        "If we apply several filters to a multi-channel input, then\n",
        "each filter will also have multiple channels. The number of channels in\n",
        "the output is given by the number of filters that were used.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/image-channel-inputs-2.PNG?raw=1' width='800'/>\n",
        "\n",
        "let’s suppose that we’ve made a convolution layer with 5\n",
        "filters, each 5 by 5. Then every output it produces will have 5 channels.\n",
        "If the next layer is also a convolution layer, and we say that we want 2\n",
        "filters that are 3 by 3, Keras will automatically know to make each filter\n",
        "5 channels deep, since that’s what’s coming out of the previous layer.\n",
        "In short, the number of channels in each filter is equal to the number\n",
        "of channels in the input, which in turn is the number of filters used in\n",
        "the previous convolution layer.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/image-channel-inputs-3.PNG?raw=1' width='800'/>\n",
        "\n",
        "Let’s make a convolution layer with 15 filters, each 3 by 3.\n",
        "\n",
        "```python\n",
        "convolution_layer = Conv2D(15, (3, 3))\n",
        "model.add(convolution_layer)\n",
        "```\n",
        "\n",
        "In practice, we usually do this in one step.\n",
        "\n",
        "```python\n",
        "model.add(Conv2D(15, (3, 3)))\n",
        "```\n",
        "\n",
        "Now that we’ve covered all the background, let’s construct a 2D convolution\n",
        "layer.\n",
        "\n",
        "```python\n",
        "model.add(Conv2D(16, (5, 5), activation='relu', strides=(2, 2), padding='same'),\n",
        "            input_shape=(image_height, image_width, 1)))\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* **padding**:To apply zero padding, we set the optional argument padding to the\n",
        "string ′same′, meaning “make the output the same size as the input.”\n",
        "The default value of padding is the string ′valid′, which means “only\n",
        "place the filter where there is valid data available.” This is a longwinded\n",
        "way of saying, “no padding.”\n",
        "\n",
        "* **strides**:we set the stride values to (2,2), then our output\n",
        "will be half the width and height as the input.\n",
        "\n",
        "* **input_shape**: a CNN layer wants the input_shape to describe not\n",
        "the shape of the whole data set, but just one sample.Therefore, the value of input_shape is the list (28,28,1), describing\n",
        "one image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THq7ZQDj7dyt",
        "colab_type": "text"
      },
      "source": [
        "## Using Convolution for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIoZZVnv7fM8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxFWDW9L3Eni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}