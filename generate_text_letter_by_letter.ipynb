{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate-text-letter-by-letter.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxnf1H5yMZx3g2l2fCfAu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning--from-basics-to-practice/blob/24-keras-part-2/generate_text_letter_by_letter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If2o8XvP073S",
        "colab_type": "text"
      },
      "source": [
        "# Generate text letter by letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGdb0KPY1ipG",
        "colab_type": "text"
      },
      "source": [
        "We’re going to use an RNN to generate brand-new text. We’ll train a little RNN using three collections of Sherlock Holmes short stories by Arthur Conan Doyle.\n",
        "\n",
        "Taken together, there’s a little over 304,000 words. Many of these words are used repeatedly, of course. There are a bit under 29,000 unique words, including many proper nouns such as the names of characters and places.\n",
        "\n",
        "A reasonable approach is to think of the text as a collection of words.\n",
        "We can then train the RNN on how words follow one another. Then we\n",
        "can start with some words, and let the RNN tell us which word should\n",
        "come next. Then we’ll take our starting bunch, plus the new word at the\n",
        "end, and have the RNN tell us which word should follow. Continuing\n",
        "the process, we can keep feeding back to the RNN the most recent set\n",
        "of words, and it would keep giving us a new word to follow.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/generate-text-1.png?raw=1' width='800'/>\n",
        "\n",
        "Our entire deep network for generating new Sherlock\n",
        "Holmes data. Our input is a list of 40 sequential characters. The characters\n",
        "go into two RNN layers. Each contains a single LSTM cell with 128\n",
        "elements of memory. The output of the second LSTM is given to a dense\n",
        "layer of 89 neurons, which predicts the probability of each character. The\n",
        "most probable character is the network’s result. The small box at the top\n",
        "of the first layer’s icon tells us that it returns an output for every input, and not just the final result.\n",
        "\n",
        "Our input consists of a string of 40 characters. To create the training\n",
        "set, we chopped up the original source material into about a half-million\n",
        "overlapping strings of 40 characters, starting every third character.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/generate-text-2.png?raw=1' width='800'/>\n",
        "\n",
        "To create new text, we produce a “seed” by picking a random starting\n",
        "point in the text, and then extract the next 40 sequential characters\n",
        "from there. We give the seed to the network and it produces a new\n",
        "character. That new character goes on to the end of the seed, and the\n",
        "first character is dropped, giving us a new 40-character seed to use as\n",
        "input to produce the next character.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/generate-text-3.png?raw=1' width='800'/>\n",
        "\n",
        "The notebooks for this section contain all the code for making new text, either letter by letter or word by word.\n",
        "\n",
        "For variation, this time we packaged up each of the steps into its own procedure. Then when we’re ready to make text, we just call some of those procedures and let them do their work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upnRrIk_1CPd",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3giPu-d1CgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import random\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqaA0GpE6F4W",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOL_M8HQ6OPZ",
        "colab_type": "text"
      },
      "source": [
        "Our first step is to read in the source text. We replaced multiple spaces\n",
        "with single spaces, and removed newline characters since they don’t\n",
        "have any semantic meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm5ZHwnL7Q_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3c08ebe2-17a3-4c3c-af29-8b04c444bd9a"
      },
      "source": [
        "def get_text(input_file):\n",
        "  # open the input file and do minor processing\n",
        "  file = open(input_file, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "\n",
        "  # replace newlines with blanks, and double blanks with singles\n",
        "  text = text.replace('\\n', ' ')\n",
        "  text = text.replace('  ', ' ')\n",
        "  print(f'corpus length: {str(len(text))}')\n",
        "\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV5d7LXE8TV7",
        "colab_type": "text"
      },
      "source": [
        "Now we have to chop up the input into overlapping windows. We\n",
        "need to pick the window size and how much they overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfdFlTk67_hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_fragments(text, window_length):\n",
        "  # make overlapping fragments of window_length characters\n",
        "  fragments = []\n",
        "  targets = []\n",
        "  for i in range(0, len(text) - window_length, window_step):\n",
        "    fragments.append(text[i: i + window_length])\n",
        "    targets.append(text[i + window_length])\n",
        "  print('number of fragments of length window_length=', window_length, ':', len(fragments))\n",
        "\n",
        "  return (fragments, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZyN3uAS_JQQ",
        "colab_type": "text"
      },
      "source": [
        "Since our network wants numbers, not letters, we’ll assign a unique\n",
        "number to each letter. To make it easy to go back and forth, we’ll make\n",
        "two dictionaries. One is keyed on characters and returns their number,\n",
        "and the other is keyed on number and returns their character.\n",
        "\n",
        "We’ll call the number an “index.” We can get the total number of unique\n",
        "characters by using Python’s set() operation. Just for general tidiness\n",
        "we’ll sort that list before using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKL18by4-0Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dictionaries(text):\n",
        "  unique_chars = sorted(list(set(text)))\n",
        "  print(f'total unique chars:{str(len(unique_chars))}')\n",
        "\n",
        "  char_to_index = dict((ch, index) for index, ch in enumerate(unique_chars))\n",
        "  index_to_char = dict((index, ch) for index, ch in enumerate(unique_chars))\n",
        "\n",
        "  return (unique_chars, char_to_index, index_to_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNwxSw16_9Dk",
        "colab_type": "text"
      },
      "source": [
        "Now we want to turn our samples and targets into one-hot vectors.We’ll use one-hot encoding for the samples here as well because we want each letter to be a feature in our data. That feature will have as many time steps as\n",
        "there are unique characters in our data. They’ll all be 0 except for a 1\n",
        "corresponding to the character being represented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRuJJp3rAZAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_training_data(fragments, window_length, targets, char_to_index, index_to_char):\n",
        "  #  Turn inputs and targets into one-hot versions\n",
        "  X = np.zeros((len(fragments), window_length, len(char_to_index)), dtype=np.bool)\n",
        "  y = np.zeros((len(fragments), len(char_to_index)), dtype=np.bool)\n",
        "\n",
        "  for i, fragment in enumerate(fragments):\n",
        "    for t, char in enumerate(fragment):\n",
        "      X[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[targets[i]]] = 1\n",
        "\n",
        "  return (X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ffkt4-AZi5",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tE8u5BQB5WF",
        "colab_type": "text"
      },
      "source": [
        "Now let’s build the model. After a little playing around, we chose the\n",
        "simple deep model.It’s just two LSTM layers and a single\n",
        "Dense layer. The first LSTM has return_sequences=True, because\n",
        "it feed another LSTM. The second one produces a single output, which\n",
        "will lead us to the letter the network is predicting. \n",
        "\n",
        "To get that letter, we use a Dense layer with one neuron per letter, and a softmax output. This will give us the probability of each character being the next one.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/generate-text-model-1.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8RKsA2fDJoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(window_length, num_unique_chars):\n",
        "  '''\n",
        "  Two layers of a single LSTM cell with 128 elements of memory,\n",
        "  then a dense layer with as many outputs as there are characters (89)\n",
        "  We'll train with the RMSprop optimizer. Some experiments suggest that\n",
        "  a learning rate of 0.01 is a good place to start.\n",
        "  '''\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128, return_sequences=True, input_shape=(window_length, num_unique_chars)))\n",
        "  model.add(LSTM(128))\n",
        "  model.add(Dense(num_unique_chars, activation='softmax'))\n",
        "\n",
        "  optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_QWvkPFAnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}